# 面部识别和对抗性攻击

> 原文：<https://medium.datadriveninvestor.com/facial-recognition-adversarial-attacks-8053c6c532b4?source=collection_archive---------9----------------------->

## 通俗地说

![](img/b8877f67b2eff863a43a6ebc899e2087.png)

# 面部识别通过以下方式进行—

1.  **人脸检测** —位置、宽度和大小
2.  **人脸对齐** —正面视图对姿态不敏感
3.  **人脸嵌入** —尽可能多的图像被送入深度学习算法(代表人脸的 n 维向量)，以唯一识别你的脸
4.  **人脸识别/验证** —验证和识别不同姿势、光照、年龄、条件、遮挡、性别、情绪下的个人

# 对面部识别系统的攻击类型—

> L **性欲攻击**

攻击者试图伪造一张脸——印刷的、剪纸形状的脸、数字的或面具的脸、在识别系统前设置视频(iPad)等。

***分辨率:*** 在约 45K 真实和欺骗照片和视频的自定义数据集上训练模型

> [**对抗性攻击**](https://blog.ycombinator.com/how-adversarial-attacks-work/)

对抗性攻击就像机器的视错觉——攻击者故意设计了一个机器学习模型输入，以从您的模型中获得错误的结果。

例子有—

*   机器的视错觉
*   图像中的小增量—决策边界
*   自动驾驶汽车
*   在现实世界中，对抗眼镜可以欺骗面部识别系统
*   不同的架构和不同的训练集

***可能的解决方案:*** 用你的机器学习系统去攻击你的另一个机器学习系统，因为它们被训练的数据集是不同的。

> **黑盒对抗性攻击**

在这类攻击中，攻击者只能有限地访问模型(有限的查询和信息)。

***可能的解决方法:*** 训练你的 ML 系统对对抗性攻击不敏感(给出戴对抗性眼镜人脸的算法例子。这是一个蛮力解决方案，我们简单地生成许多对立的例子，并明确地训练模型不被它们中的每一个愚弄。)