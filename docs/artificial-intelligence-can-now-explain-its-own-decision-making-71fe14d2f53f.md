# 人工智能现在可以解释自己的决策

> 原文：<https://medium.datadriveninvestor.com/artificial-intelligence-can-now-explain-its-own-decision-making-71fe14d2f53f?source=collection_archive---------10----------------------->

![](img/9fec0319833b4cfc5778815d6a8ea5e3.png)

Picture: mikemacmarketing

人们对未知感到恐惧。因此，自然地，人工智能(AI)尚未被广泛采用的一个原因可能是因为机器决策背后的基本原理仍然未知。

当人们不知道他们从哪里来时，怎么能相信他们的决定呢？

这被称为人工智能的黑匣子——需要被破解的东西。随着技术继续在日常生活中发挥越来越重要的作用，并改变劳动力中的角色，算法背后的伦理已经成为一个热门话题。

医疗从业者被认为是最先从人工智能和深度学习技术中受益的人之一，人工智能和深度学习技术可以轻松扫描图像和分析医疗数据，但只有当人们理解如何得出结论时，决策算法才会受到信任。

关键思想家警告说，软件背后的偏见和偏见可能会被算法强化，但一家总部位于纽约的技术公司认为，它正在引领建立可问责的技术。

IBM 声称，它已经采取了重大步骤，通过一项带来人工智能透明性的软件服务，打破了块盒子。

该服务旨在提供对人工智能如何做出决策的洞察，它还可以自动检测偏差，并在做出决策时解释自己，以及建议更多数据纳入模型，这可能有助于消除未来的偏差。

IBM 之前部署了一个人工智能来帮助 IBM Watson 进行决策，该系统为临床医生提供了基于证据的治疗计划，该计划优化了自动化护理管理和患者参与定制计划。

专家们很快对这个模型产生了怀疑，因为它没有解释决策是如何做出的。沃森有助于医疗诊断，并加强医生的决定，但这种充满希望的技术永远不会取代医生。当沃森提供与医生一致的分析时，它被用作一种强化措施。当沃森不同意时，这是错误的。

但该公司目前尚未命名的最新创新似乎解决了沃森的不足。或许把它命名为夏洛克会更合适。

增加的透明度不仅体现在决策过程中，还体现在模型的准确性、性能和公平性的记录可以方便地进行跟踪和召回，以满足客户服务、法规或合规性要求，例如 GDPR 合规性。

在宣布这一人工智能的同时，IBM Research 还将发布一个开源的人工智能偏见检测和缓解工具包，推出工具和教育，以鼓励围绕解决人工智能偏见的全球合作。

这包括一个算法、代码和教程库，将为学者、研究人员和数据科学家提供工具和知识，以便在他们构建和部署机器学习模型时集成偏差检测。

虽然其他开源资源只专注于检查训练数据中的偏差，但 IBM AI Fairness 360 toolkit 将有助于检查和减轻人工智能模型中的偏差。

IBM 认知解决方案的 SVP 大卫·肯尼说:“IBM 在为新人工智能技术的开发建立信任和透明原则方面引领了行业。

“是时候将原则转化为实践了。我们正在为使用人工智能的企业提供新的透明度和控制，这些企业面临着任何有缺陷的决策带来的最大潜在风险。”

这对医疗从业者来说意味着什么？这项新技术的实施可能会带来一系列问题，因为政策仍需跟上技术的发展。谁对错误诊断后的问题负责:医生还是 IBM？在一个经过验证的正确诊断记录之后，一个人怎么会违背软件呢？如何证明直觉是正当的？