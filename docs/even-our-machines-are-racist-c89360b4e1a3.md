# 机器学习中的偏见和对客观现实建模的困难

> 原文：<https://medium.datadriveninvestor.com/even-our-machines-are-racist-c89360b4e1a3?source=collection_archive---------20----------------------->

![](img/f09fa6a27df86f0990913ed86175e3d9.png)

Example of a Racist Ethereal Levitating Machine Brain

在美国身份政治和部落主义的分裂世界中，甚至我们的机器都是权力结构的歧视性工具。即使在没有道德、没有灵魂的机器人的环境中，客观性及其强烈的吸引力也会成为牺牲品！

好吧，这可能有点夸张，但是数据的收集和使用确实存在问题。在创建算法的过程中去除人的因素并不足以解决问题。

让我们首先考虑客观机器的想法。这似乎很简单。这个与道德无关的逻辑盒子通过数学和数据分析构建了一个表达和解决方案的世界。身份政治、种族主义、同性恋恐惧症和微观进步的影响和社会压力不会扰乱机器的思维，因为它没有扰乱思维的能力。机器也不会受注意力分散或懒惰的影响。这些是以独特的文化历史和生物构成为特征的人类问题，而机器不具备这些特征。

因此，客观性的梦想已经被输出到机器上。

**我们在技术发展方面处于什么位置？**

当今天提到 AI 时，它通常是指狭义的人工智能(AI)。这是指机器模仿“学习”和“解决问题”的认知功能，但仅限于解决特定类型的预先确定的目标。

一些例子包括 [AlphaGo](https://deepmind.com/blog/alphago-zero-learning-scratch/) 或者像星巴克一样根据服务相关行业的需求制定时间表。狭义人工智能通过应用算法来实现特定的目标。例如，配方就是一种算法。配料是投入，目标是一顿令人愉快的饭。

一些评论家，如《数学毁灭的武器》一书的作者凯茜·奥尼尔，强调了与今天过度关注错位目标的狭隘模型相关的不平等。

一个例子是星巴克使用的时间安排技术，纽约时报的这篇题为“[除了朝九晚五](https://www.nytimes.com/interactive/2014/08/13/us/starbucks-workers-scheduling-hours.html)之外的任何工作”的文章强调了这一点这篇文章记录了工作时间的不稳定和不可预测的短时间通知，这种通知是由一种算法建立的，该算法计算销售模式和其他数据，以准确确定何时需要咖啡师。这篇文章强调了不稳定的时间安排的外部性，以及它给低收入个人带来的困难，同时引发了一种有害的反馈循环，损害了周围的个人和员工的子女。主要问题是对员工健康、福利和社会进步的考虑受制于底线。

这突出了“对齐问题”的一个简单例子，以及在这个早期阶段根据我们想要生活的社会讨论我们希望我们的机器做什么的重要性。

机器学习(ML)是人工智能的下一个迭代和子集，基于系统可以以大大超过人类的速度处理和识别数据模式的想法。ML 的与众不同之处在于它的交互性和适应性，能够实时收集数据，并根据这些输入调整响应和后续功能。ML 是狭义 AI 算法的进化。在 ML 中，算法是通过对数据集的研究构建的，这些数据集能够破译模式并自行做出预测。

AI 和 ML 之所以成为可能，是因为有收集和存储大量数据的能力。与数据收集和存储相关的隐私挑战是众所周知的。它们包括保护数据集和筛选出个人身份信息的难度增加，取消数据识别，以及通过比较数据集之间的数据来重新识别个人的可能性。

在

因此，这篇文章鼓励仔细监测和审查现有的人工智能，继续考虑我们想要保护的其他价值，如隐私、尊重和人类尊严。

**数据集中的偏差**

在凯茜·奥尼尔的书《数学毁灭的武器》中，她反复强调由糟糕的算法目标导致的不公平循环，同时哀叹模型中缺乏透明度和假设的定期更新。

她举的一个例子是使用信用评分作为确定个人就业能力的工具。她强调了这对生活在贫困中的人们的强化效应。这种恶性循环表明，信用不良的穷人无法找到工作，这进一步加剧了借贷需求和还款能力，从而导致信用更差，找到工作的可能性进一步降低。这是狭义 AI 的一个例子。

机器学习的出现提供了通过实时包含更多数据来不断更新算法的机会，并因此提供了更大的机会来改进受审查的变量。当机器学习的逻辑无法用人类语言表达时，关于决策不透明的紧急问题仍然存在。这给像 GDPR 这样寻求通过透明度要求来消除偏见和不平等的管理机构带来了复杂性。抵制来自科技公司，它们将自己的算法视为商业秘密，担心竞争对手窃取。

这进一步加剧了与偏见相关的担忧，无论是出于有意识还是潜意识的原因，这些偏见都存在于我们的数据集中。现在我知道我答应了种族主义机器，但问题不一定是机器，而是他们构建算法的数据。

例如，几年前谷歌开发了一个图像识别软件，它的大部分学习都是在白人数据集上进行的。结果是一场混乱，谷歌照片的自动标签系统错误地识别了 Jacky Alciné和一个标签为“大猩猩”的朋友

这显然是可怕和可恶的。另一个例子将显示一个更良性的表示。在算法的构造中，种族的使用通常被认为是不道德和不好的做法。邮政编码通常不会被这样看待。但在操作上，它们代表了根深蒂固的种族隔离的历史遗迹，并对特定地区的人类行为历史表达了一种观点，这种观点让后代面临同样的结果。

一旦机器被赋予创造算法的任务，这个问题就不会被免除。如果训练数据反映了社会偏见，一个算法将会包含这些偏见。种族、性别和其他偏见可以被机器破译，机器的吸引力在于从在场的人身上推断出不存在的属性。正如加州大学伯克利分校教授大卫·奥本海默所说，“即使它们不是为了歧视而设计的……如果它们以完全理性的方式复制社会偏好，它们也会复制那些形式的歧视。”

继续这种推理，这种担心尤其有害，因为由此产生的歧视是无意的，因此很难识别。解决办法可能是使识别特征明确，以便对它们进行控制。

在一篇名为*大数据的不同影响*的伟大[法律评论文章](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899)中提出的另一个有趣的问题解决了关于算法不透明的根本问题，并借鉴了就业背景下歧视的历史先例。作者发现的问题被称为“屏蔽”，它允许数据挖掘员工进行歧视，同时反驳这种说法，证明歧视是极其困难的。

更重要的一点是，在所有情况下，只有在可以获得揭示偏见的信息的情况下，分析师才能纠正各种形式的歧视。

正如谷歌广告研究的作者阿米特·达塔、阿努帕姆·达塔和迈克尔·卡尔·特尚茨所写的，“一个算法的非道德地位并不能否定它对社会的影响。”

**联邦立法**

考虑到所有这些，当美国考虑关于这个主题的联邦立法时，一些公共利益组织发布了一些指导原则:

1.隐私保护必须是强有力的、有意义的和全面的

2.数据实践必须保护公民权利，防止非法歧视，促进平等机会

3.各级政府应该在保护和实施隐私权方面发挥作用

4.立法应提供对侵犯隐私的补救措施

依次考虑每一点，一项全面的立法是受欢迎的，并将建立一套统一的强制措施，以取代联邦化的系统，在这种系统中，每个州都有监管科技行业的任务。这套统一的标准将允许各州之间更大的交易流动性，并在全国范围内就一套明确的标准给予公司适当的通知。各级政府都必须参与执法的想法似乎同样显而易见。将执行标准完全放在联邦政府身上是不现实的，而且会使政府负担过重。

第二点是一个高尚的努力，并试图编纂隐私立法中的自由价值观。正如已经指出的，困难是双重的。首先，算法存在固有的不透明性，其目标和输入作为商业秘密受到保护。第二个困难来自机器学习的复杂性，以及无法用人类能够理解的术语来描述结果。也许清楚地标注输入是解决这一困难的最佳方式，并允许数据分析师控制偏差，同时为监督组织提供透明的画面。

然而，这一点似乎完全违背了个人隐私，我们已经看到最近加密和假名化的激增。这是为了解决关于利用跨平台数据创建档案的能力的问题，这些数据可用于极端形式的定向营销和政治广告。

最后一点，即纠正权，很重要，它赋予个人对其数据的某种代理和控制权，并提供一种纠正错误的机制，同时对数据处理者进行检查，激励充分的数据保护和打击滥用数据的行为。

人工智能和机器学习提供了许多好处和挑战。困难在于创造一种环境，既保证充分重视隐私，又不抑制创新。