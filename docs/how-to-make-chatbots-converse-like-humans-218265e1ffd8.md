# 如何让聊天机器人像人类一样交谈！

> 原文：<https://medium.datadriveninvestor.com/how-to-make-chatbots-converse-like-humans-218265e1ffd8?source=collection_archive---------47----------------------->

[![](img/9fe987d7ed0d1c2f21f416102a436623.png)](http://www.track.datadriveninvestor.com/1B9E)

人们对聊天机器人的兴趣与日俱增。随着越来越多的人熟悉聊天机器人，对高质量机器人的需求也在增加。机器人不再是问答机器。他们必须非常优秀。现在，你如何确定一个机器人是好是坏？嗯，你可以说一个好的机器人表现得更像人类。这是真的，然而，有必要量化机器人的类人行为。

这是一个量化机器人类人行为的尝试。虽然可能有许多其他因素，但这里列出的因素被认为是主要的。一个机器人需要:

*   稳定的
*   聪明的
*   迷人的
*   在旅途中学习
*   有个性

这些是不可量化的。我们必须更深入地挖掘，将它们分解成更小的因素，然后尝试量化。

# 稳定的

![](img/4c96ca9cae48b782c1eabda0c608a0a2.png)

什么时候你认为一个机器人是稳定的？当它没有给出错误的答案/当它没有给用户错误的方向时？

如何构建一个稳定的机器人？一些指导方针是(我正要称它们为规则，但是因为我需要更多的自信来称它们为规则而忍住了):

识别正确的意图并构建意图。一般来说，人们倾向于结合多种意图来简化 bot 构建过程。但是，随着机器人的发展，这只会导致不稳定。

避免在同一个机器人上添加两个相似的意图(例如:“买一个苹果”、“买一个汉堡”是两个相似的意图)。类似的意图增加了不稳定性

不要加载超过其能力的机器人。更多的意图意味着达到正确意图的可能性更小。尝试达到最佳的意图数量。

我们如何衡量机器人的稳定性？从表面上看，这是一个棘手的问题，的确如此。衡量一个机器人的稳定性需要一套好的通用和专用测试用例。通用测试用例是任何机器人通用的，构建和使用通用测试用例是一个好的实践。特定的测试用例是专门为机器人设计的。特定测试用例的输出可以用来衡量机器人的稳定性。好的测试用例会产生稳定的机器人。因此，在构建这些测试用例时要遵循最佳实践。

# 聪明的

![](img/9c5ed453b9b7aed0792f8ef58bd1cb3c.png)

什么时候你认为机器人是聪明的？当它的行为不像一个愚蠢的。没错！机器人不应该重复自己；它不应该问明显的问题；在某些情况下，即使在不同的会话中，机器人也应该记住一些信息。对机器人的要求是不是太多了？不是的！被普通人认为是愚蠢的机器人将很快停止存在。因此，将机器人的聪明程度与普通人相匹配是很重要的。

上下文处理是确保机器人智能的一种重要方式。有许多处理上下文的方法。一种经常适用的方法是意图聚类。在这种方法中，意图被分组到具有一些公共槽的集群中。通用插槽的命名在各种意图中都是相同的。簇中具有相同名称的槽携带相同的值。我们还可以定义在所有意图中通用的全局槽。这些可能是像雇员 id，姓名等插槽。

在构建机器人时，转换上下文也是一个重要的方面。它应该能够处理在两个上下文之间切换的简单情况。通过要求用户澄清，可以处理两个以上的上下文。这应该是处理歧义的一种相当公平的方式。

与上下文相关的假设必须确保稳定性不受损害。因此，在回复中包含完整的详细信息是一个很好的做法。

# 迷人的

![](img/b7eafe4be1c90ed38fa06a756eaab191.png)

两个人之间的典型对话有多少次互动？在两个朋友聊天的情况下，对话可能是无止境的(意味着互动甚至可以达到几千次)。由于这种类型的对话非常不明确，而且很难模拟，所以让我们先来看看更有条理、更容易模拟的职业互动。在一次专业对话中，互动的次数大约是 10-20 次。如果我们的目标是每次对话进行 10 次互动，机器人必须采取一些积极的措施来引导对话。不仅如此，积极主动还必须有意义。如果不是，这将是对机器人智能的妥协。

为了更聪明地主动出击，机器人必须识别用户的兴趣，并在意图实现后相应地触发有意义的下一组交互。这看起来类似于亚马逊网站幕后的推荐引擎——当你买一本书时，你的足迹会被捕捉，并被转换为一个向量，这些推荐是通过查看平行向量得出的。以类似的方式，当用户与机器人交互时，它必须识别对话向量，寻找平行向量，并相应地预测下一个可能的意图或意图，并驱动对话。

这里可以使用强化学习技术来预测用户可能感兴趣的下一个可能的意图。在这种方法中，确定模型的回报至关重要。奖励，可能是用户采取的下一步行动，可能是点击按钮，对机器人的预测做出负面反应等。好的奖励计算导致更好的学习模型

# 有个性

![](img/6888086d03bdb5abe550253b66c5f6cb.png)

机器人需要一种个性，这样它们才能像人一样，有个性。每个机器人应该有自己的身份，并应避免落入一个通用桶。如今，大多数机器人都被贴上了某种助手的标签。机器人可以超越这一点。机器人可以是特定领域的专家、分析师、观察者等等。而这一切仅仅发生在企业领域。如果机器人开发者忽视赋予机器人个性，他们很快就会出局。

# 在旅途中学习

![](img/53011042ddd20dfc45b78892f4452073.png)

人类在交谈中学习。让我们以儿童为例，他们知道语言，但没有知识。当他们与成人互动时，信息从成人流向儿童。例如，一个成年人告诉一个孩子，人类吸入氧气，呼出二氧化碳。现在，给定孩子对成人的信任水平，孩子会将该信息存储为要验证的简单信息，或者甚至会丢弃该信息。假设孩子对成人有很大的信心，他/她可以把它当作事实，并在他/她的大脑中作为一条规则来写。下一次，你问孩子同样的问题，孩子从知识库中提取信息，做出反应。以类似的方式，机器人应该有能力从对话中学习，并增强其知识库。

一旦孩子长大了，积累了更多的知识，他/她甚至会在谈话中挑战其他人。一个未来派机器人还应该致力于发展一种技能，在这种技能中，它可以根据自己的知识和逻辑思维能力挑战用户的知识。从发展速度来看，争论的机器人似乎不会太远。

作者:JayaPrakash Kommu，解决方案架构师和联合创始人 Smartbots.ai

[https://www.linkedin.com/in/jayaprakash-kommu-a1b7a911/](https://www.linkedin.com/in/jayaprakash-kommu-a1b7a911/)