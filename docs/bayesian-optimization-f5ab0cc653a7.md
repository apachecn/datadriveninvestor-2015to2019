# 贝叶斯优化

> 原文：<https://medium.datadriveninvestor.com/bayesian-optimization-f5ab0cc653a7?source=collection_archive---------3----------------------->

## 什么是贝叶斯优化？如何调整深度神经网络的超参数？

***深度神经网络超参数整定贝叶斯优化的直观解释***

![](img/3a9aa41ae753a0fe6ab1a5aa97da3ae7.png)

*我们先来了解一下贝叶斯统计*

过去，如果一个板球队在某个场地的 12 场比赛中赢了 5 场。在五场比赛中的三场之前，雨下得很大。一场比赛因下雨而输了。如果下雨，这个队赢下一场比赛的可能性有多大？

我们想知道 p(赢|下雨)=？

p(中奖)= 5/12=0.417

p(赛前下雨)=4/12=0.33

p(下雨了|赢了)=3/5=0.6

应用贝叶斯定理

> p(A|B) = p(B|A) x p(A)/p(B)

p(赢|下雨)= p(下雨|赢)x p(赢)/p(赛前下雨)

0.600 x 0.417/0.333 = 0.75

我们现在可以预测，万一下雨，该队有 75%的可能性赢得这场比赛。

贝叶斯统计为计算未来事件的可能性提供了一种数学方法。未来事件是根据先前事件的知识来预测的。

[](https://www.datadriveninvestor.com/2019/01/23/deep-learning-explained-in-7-steps/) [## 深度学习用 7 个步骤解释-更新|数据驱动的投资者

### 在深度学习的帮助下，自动驾驶汽车、Alexa、医学成像-小工具正在我们周围变得超级智能…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2019/01/23/deep-learning-explained-in-7-steps/) 

贝叶斯统计始于一个先验信念，它被表达为一个先验分布。然后用新的证据更新这一点，以产生后验信念。后验置信也是一种概率分布。

***现在我们换个话题，谈谈超参数调谐。***

在深度学习中，我们执行以下步骤

*   创建模型，
*   将超参数应用于模型
*   在训练数据集上训练模型。
*   评估模型在测试集或验证集上的性能。
*   微调模型以获得最佳精度

[为了微调模型的性能](https://medium.com/analytics-vidhya/improving-performance-of-a-neural-network-22776fde2089)其中一项技术是超参数调整。

**神经网络的超参数示例**

*   隐藏单元的数量
*   辍学
*   纪元
*   批量
*   学习率
*   【计算机】优化程序

深度学习模型更加深入和复杂，需要调整多个超参数。这使得超参数调整在计算上非常昂贵。

我们可以使用以下方法优化神经网络的超参数

*   手动搜索
*   [网格搜索](https://medium.com/datadriveninvestor/tuning-artificial-neural-network-b028dcc3b9d0)或
*   随机搜索或
*   贝叶斯优化。

**网格搜索**对指定参数进行彻底搜索。**网格搜索是网格中所有指定超参数的笛卡尔乘积**。这里，我们取一个超参数，并保持所有其他超参数不变，以最小化损失。对超参数的所有组合都这样做。**这在计算上非常昂贵。**

**随机搜索**使用超参数的统计分布。由于超参数是随机选择的，因此并不是每个参数组合都要尝试。随着超参数数量的增加，随机搜索是更好的选择，因为它可以更快地找到好的组合。**就计算成本而言，随机搜索比网格搜索在超参数优化方面更有效。**

随机搜索和网格搜索方法不学习以前的结果。他们**完全*不了解*过去的评价。**因此，我们花费大量时间评估可能不是很有用的超参数

**贝叶斯优化**是超参数优化问题的一个优雅的解决方案

**贝叶斯优化结合了关于超参数的先验数据，包括模型的准确性或损失。先验信息有助于确定模型超参数选择的更好近似。**

它提供了一种建模不确定性的原则性方法。这使得探索和开发在搜索过程中自然平衡。

**贝叶斯优化是基于序列模型的优化** (SMBO)算法。SBMO 用于适应度函数评估昂贵的应用中。超参数调整是一项计算量很大的任务，是贝叶斯优化的理想选择

基于贝叶斯模型的优化的整个概念是减少目标函数需要运行的次数。这是通过仅选择最有希望的超参数集进行评估来实现的。超参数的选择基于之前对评估函数的调用。

**下一组超参数是基于称为替代**的目标函数模型选择的。

这里我们的目标函数可以是最小化神经网络中的损失或者最大化模型的准确性

贝叶斯优化的步骤

1.  **创建一个我们想要探索的超参数域**
2.  **创建一个接受超参数并输出分数的目标函数。**分数可以是我们希望最小化的损失或错误，也可以是我们希望最大化的准确性。目标函数将使用传递的超参数创建代理深度学习模型。它将输出模型的精度或模型中的误差。
3.  **创建一个标准，称为选择函数，用于评估下一步从代理模型中选择哪个超参数**
4.  **由分数和超参数对组成的历史被算法用来更新代理模型**

**结论:**

贝叶斯优化是一种有效的技术，可以找到超参数的最佳值，以建立最佳的深度学习模型。与网格搜索和随机搜索不同，贝叶斯优化不能孤立工作。它使用关于超参数和模型精度或损失的先验信息，对超参数选择做出明智的决定。计算效率非常高。

## 参考资料:

[https://towards data science . com/a-基于贝叶斯模型的概念解释-机器学习的超参数优化-b8172278050f](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)

[http://proceedings.mlr.press/v37/snoek15.pdf](http://proceedings.mlr.press/v37/snoek15.pdf)

[https://papers . nips . cc/paper/4443-algorithms-for-hyper-parameter-optimization . pdf](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)