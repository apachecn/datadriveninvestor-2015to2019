# 可解释的人工智能:我们需要知道算法为什么做出决定

> 原文：<https://medium.datadriveninvestor.com/explainable-ai-we-need-to-know-why-algorithms-make-their-decisions-1f5d45cccc84?source=collection_archive---------22----------------------->

![](img/ada36ec7e514fa0fe47e11057ba0d856.png)

人工智能(AI)将传统的编程从预设规则转变为机器编写自己的推理。通过人工智能连接各个点，我们可以找到解决问题的新方法，大规模运行更多流程，并减少人为错误的机会。

随着实施的增加，人工智能将开启工作的“最后一英里”——传统自动化无法解决的任务。但是，我们真的能信任这些机器来指导重大商业决策吗？毕竟，如果没有适当的数据和训练，人工智能可能容易出现[意想不到的偏差](https://medium.com/@Srivastava_SNJ/ais-bias-problem-and-what-we-need-to-do-to-solve-it-9193aa0d8222)。我们怎么知道机器做出了正确的选择？

理解人工智能的推理是企业中许多决策者最关心的问题。在不知道算法为什么做出决定的情况下，我们会面临不被采纳以及员工和客户不信任的风险。此外，在医疗保健、保险或银行等一些市场，采用受到阻碍，因为监管机构要求自动化是可解释的。

**可追溯性驱动可解释性**

例如，我们知道深度神经网络非常强大，但它们更类似于黑盒，因为我们无法解释它们的决定。

然而更重要的是如何使用人工智能构建应用程序。将决策分解为多个组成部分并提供每个组成部分背后的事实的可视路径的能力可以使 AI 应用程序背后的过程更加透明。这种提供面包屑的能力——通过认知过程的各种逻辑图——可以提供使答案更容易解释所需的保证。

例如，我们可能还没有完全理解引发特定突触通过我们的神经系统的化学和电信号。但是我们可以追踪左手的疼痛信号，拉起我们的衬衫袖子，看到炎症，并判断出这是否是昆虫叮咬。精确定位结论来源的能力推动了可解释性。

**可解释性推动更好的客户体验、合规性和采用。**

理解 AI 的逻辑和推理不仅仅是双重检查机器的工作。我们必须能够展示是什么让我们做出了那个特定的决定。例如，在贷款审批中，如果一个基于人工智能的系统建议拒绝一个小企业贷款的申请，那么该应用程序需要以一种允许用户跟随决策回到触发拒绝的特定步骤的方式来构建；例如，资产负债表上的脚注表明高于可接受的风险。

现在，银行可以点击并钻过决策路径的层次结构，找到导致答案的实际脚注，而不仅仅是以是或否结束。事实的清晰可见性导致各方之间更多的理解、更好的治理和更好的客户体验。

重要的是，这种方法更加合规，因为导致自动决策的准确信息现在可以提供给监管机构，而不是黑盒算法驱动的答案。

解释人工智能是必要的，不仅是为了应对监管审查，也是为了帮助采用。就像员工和公民追随他们信任的领导者一样，如果员工和客户信任这项技术，他们将更愿意采用人工智能。

**解决可解释人工智能的关键在于应用设计**

关于人工智能中的可解释性问题，已经写了很多。解决这个问题的诀窍不在于人工智能本身，而在于如何将人工智能应用于构建人工智能应用的过程中。重要的是将任何决策分解成组成步骤。并在应用程序中为这些驱动最终答案的原子认知步骤构建点击和钻取功能。在设计应用程序时考虑到可解释性，这是成功的关键。