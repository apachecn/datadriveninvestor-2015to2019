# 部署 AI 客户端的 5 个理由

> 原文：<https://medium.datadriveninvestor.com/5-reasons-to-deploy-ai-client-side-1cb937647495?source=collection_archive---------13----------------------->

深度学习模型通常部署在服务器或 API 上，其中内容提供商使用他们的资源(服务器端)执行必要的计算。如果提供商试图将计算推给消费者硬件(客户端)，当运行最先进的神经网络时，它可能会使用过多的带宽、内存和 CPU。然而，由于最近在深度学习算法( [MobileNets](https://arxiv.org/abs/1704.04861) )以及框架( [TensorflowJS](https://js.tensorflow.org/) 和 [ONNX)方面的创新。](https://microsoft.github.io/onnxjs-demo/#/) JS)，我们能够上传只有 10MB 左右，仍然能够进行高质量推理的模型。

但是为什么有人会大费周章地缩小他们的模型，将它们导出到客户端框架，并编写新的代码来服务它们呢？有几个好处是服务器端实现无法实现的:

1.  **实时分析**:尽管客户端硬件可能很慢，但它几乎肯定比等待从 API 检索结果要快，并使实时流视频分析成为可能。例如，一个检测多人的[模型](http://aachristiansen.com/person-detector/)在我的手机上以每秒 7 帧(FPS)的速度运行，在我的电脑上以每秒 11 帧的速度运行。
2.  **离线能力**:通过将客户端深度学习模型部署到渐进式 Web App 上，用户可以在不连接互联网的情况下，将内容下载到手机上进行推理。
3.  **隐私:**与客户端模型离线工作的原因相同，用户不需要担心他们的数据通过互联网发送或存储在数据库中。
4.  **易于部署**:客户端模型对小型 Web 应用程序很有吸引力，因为从 Javascript 部署的模型可以在一个独立的静态网站上提供，完全不需要管理服务器。
5.  **成本**:客户端部署消除了服务器或无服务器功能需要执行的所有计算。发送模型文件有一次传输成本，但是接收和可能返回多媒体输入和输出数据的成本被去除了。

为了测试一个无论何时有人进入屏幕都会提醒你的简单模型，请看原文：<https://aachristiansen.com/person-detector/>

![](img/1591ad9fd943ea6cedbb687c79d400c2.png)