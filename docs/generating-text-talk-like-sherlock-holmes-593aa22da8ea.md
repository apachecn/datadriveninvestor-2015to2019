# 生成文本:带回夏洛克·福尔摩斯

> 原文：<https://medium.datadriveninvestor.com/generating-text-talk-like-sherlock-holmes-593aa22da8ea?source=collection_archive---------9----------------------->

*   然后，我的朋友罗斯告诉我，他见过他的父亲 T2
*   **深度学习**。？”“恐怕我没有看到这件事的陈述。"
*   我想我见过那个人

前面的 3 个语句是深度学习模型对夏洛克·霍尔姆斯的冒险进行训练的输出，并给出了初始单词(以粗体书写)…下面是它的工作方式

毫无疑问，深度学习是目前人工智能领域最热门的话题，它在不同的应用领域取得了成功，包括视觉，生成艺术，图像字幕，生成文本等。

其中一个著名的成果是艾写的《哈利·波特》这一章

在阅读了几个关于字符级生成模型的教程(参考文献 1-4)后，我想试着构建我自己的模型，但是首先……代码/笔记本

# 模型

我使用了一个由 2 个 LSTM 图层(每个图层有 256 个单元)和一个密集(完全连通)图层组成的模型，用于输出分类交叉熵成本和正则化丢失

什么是 LSTMs，为什么使用 LSTMs？

长短期记忆(LSTM)单元是一种用于构建递归神经网络(RNNs)的单元，该网络具有使用/记忆来自过去状态的信息的能力，并且使用它们构建的神经网络(又名语言模型)是当前用于自然语言处理的网络

# 数据

我使用了从古腾堡计划下载的阿瑟·柯南·道尔的书《福尔摩斯探案集》作为训练语料库

# 预处理

下载完这本书后，我手动删除了包含古登堡计划信息的标题和结尾，然后在代码中，我将所有英文字符转换成小写形式，并将文本拆分成长度为 100 个字符的句子，并使用一键编码对它们进行编码

# 培养

最初，我想使用 512 的批处理大小运行 100 个时期的“adam”优化器

然而，训练数据太大，无法保存在 ram 中，所以我将其分成大小为 50，000 个句子的块，并且对于 100 次，我将数据分成块，并在每个块上运行 adam 的一个步骤

# 结果

该模型可以正确地输出具有正确英语单词和标点的长句，这些长句有时像任何人类书面句子一样可理解，但大多数时候无非是幻觉。

人们可以通过注意到思考一个意义，然后选择我们的词，然后写这些词来进行推理，而我们强迫模型做的是写一个又一个没有意义概念的字符，这就像在喝醉时不得不写一篇没有主题/信息的文章

# 研究方向和改进

受 vision 中生成模型成功的启发，人们可能会想，我们能否在文本生成中复制这一成功。

在这种情况下，最常用的方法是生成对抗网(GANs)和变分自动编码器(VAEs ),不过 Ian good fellow(GANs 的发明者) reddit 帖子中的描述 gan 不适合文本任务，因为它们需要连续的空间来工作，而文本本质上是离散的，即使我们使用单词的矢量嵌入也无济于事，因为如果一个单词使用矢量 V 编码，V + dV 将映射到 V(其中 dV 是无穷小的)，而如果 I 是一个图像，那么 I + dI 是一个不同的图像(无论多么微小)

因此，已经尝试使用可以处理离散空间(例如 VAEs)的生成模型，但是 Bowman 等人 2015 年[5]进行的实验表明，具有 LSTM 解码器的变分自动编码器(VAE)的性能比更简单的 LSTM 语言模型(例如我们刚刚训练的模型)差

研究人员正在尝试的另一种方法是结合几个模型，如(VAE + CNN) [6]以及结合 VAE、RNN 和/或全连接层[7] [8]

# 参考

1.  [递归神经网络的不合理有效性](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
2.  [用 LSTM 递归神经网络在 Python 中用 Keras 生成文本](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)
3.  [使用递归神经网络创建文本生成器](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/)
4.  [使用 LSTMs 的语言建模和文本生成 NLP 的深度学习](https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275)
5.  [用于学习自然语言推理的大型标注语料库](https://nlp.stanford.edu/pubs/snli_paper.pdf)
6.  [使用扩展卷积改进文本建模的变分自动编码器](http://proceedings.mlr.press/v70/yang17d/yang17d.pdf)
7.  [用于文本生成的混合卷积变分自动编码器](https://www.aclweb.org/anthology/D17-1066)
8.  [基于具有潜在变量的生成对抗网络的文本生成](https://arxiv.org/pdf/1712.00170.pdf)