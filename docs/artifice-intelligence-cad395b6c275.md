# 人工智能

> 原文：<https://medium.datadriveninvestor.com/artifice-intelligence-cad395b6c275?source=collection_archive---------13----------------------->

[![](img/400ed1383ab7707f8056a1f0cbb0c3ed.png)](http://www.track.datadriveninvestor.com/1B9E)

**论人工智能的局限性**

![](img/7bbc3748deef7b0649dc1c3ceb0ca0bc.png)

Photo: E. Beauxis-Aussalet

赋予无生命的物质智慧，甚至生命，是人类的古老梦想。古代神话讲述了变成人类的雕像:希腊的加拉太亚或潘多拉，芬兰的伊尔玛琳的黄金新娘。不那么浪漫的神话包括希腊铁匠之神赫菲斯托斯的机械生物[ [2](https://en.wikipedia.org/wiki/Hephaestus) ， [3](https://en.wikipedia.org/wiki/Talos) ]，或者塔木德和东欧民间传说中的魔像[【4】](https://en.wikipedia.org/wiki/Golem)。

无论我们的幻想和命运有多么丰富，这些神话都远远不能描述今天的人工智能(AI)。或许除了魔像。

今天的人工智能本身并不智能。它要么通过静态规则集([符号化](https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence)或[模型驱动的人工智能](https://en.wikipedia.org/wiki/Model-based_reasoning))，要么通过重复的统计和计算([机器学习或数据驱动的人工智能](https://en.wikipedia.org/wiki/Machine_learning))。智能是否可以通过规则或计算实现，或者两者同时实现，这不是我们这里的话题。我们更倾向于关注阻碍它们的两个特征:它们的*静态*和*重复*性质。对世界的静态和重复的表征能实现智能吗？

人工智能计算的数字不是静态的，例如，它们随着对真实世界情况的测量随着时间的推移而演变。触发机器执行人为规则和规定动作的条件也不是静态的。然而，执行它们的框架是基本静态的。变量集是静态的，尽管变量的值可能会改变。可以执行的人为规则是静态的。可以触发规则的条件集是预定义的。尽管条件的状态是动态的，并且可以反映特定的环境，但是潜在条件的集合是有限的。本质上，人工智能用来代表世界的一套度量标准，以及它们的计算机化模型可以进化的可能性，都是静态的。

计算机程序中的代码行，即更新度量值或计算机化模型状态的框架，以及作为对给定情况的智能响应的一种形式执行规定的动作，也是静态的，因此被重复应用。在这样静态重复的框架内，智能能否实现？这样的框架中没有反映出什么，并且最终会产生不明智的反应？人工智能系统就像柏拉图洞穴中的囚犯[【5】](https://en.wikipedia.org/wiki/Allegory_of_the_Cave)，或者玛格丽特背叛图像的受害者[【6】](https://en.wikipedia.org/wiki/The_Treachery_of_Images)。

对于健全性检查，计算机科学家可以依赖乔治·博克斯的格言:*“所有的模型都是错误的，但有些是有用的”* [ [7](https://en.wikipedia.org/wiki/Occam%27s_razor) ，8】。模型并不代表全部真相，而是真相中有价值的部分。模型越接近真相，就需要越复杂，例如，对现实的微妙之处进行编码需要无数的参数。奥卡姆的威廉认为，省略世界复杂性的一部分是赋予我们模型力量的原因。更简单的模型更容易在实践中应用。更重要的是，偏好更简单的模型会导致制定出可以在更大范围的案例中进行测试的基本原则。更基本因而更适用、更可检验因而更可靠的原则，为我们提供了非常有价值但却是片面的真理。

乔治·博克斯(Georges Box)承认偏好更简单模型的价值和局限性:*“实际问题是【模型】要错到什么程度才会没用”* [ [7](https://en.wikipedia.org/wiki/Occam%27s_razor) ，8】。有些情况下，模型是错误的，人工智能系统是不智能的。我们的人工智能模型只是事实的一部分。它们包含的信息可能是真的，但仍不完整。使用部分表示赋予了人工智能一定的应用范围……也带来了限制。人工智能的智能既被允许，又被限制于他们模型的简化程度。承认人工智能的局限性是在人工智能提供智能响应的情况下应用人工智能的关键条件。

![](img/ca30fcb952ceecee61857c6f4f4c6497.png)

Photo: E. Beauxis-Aussalet

人工智能是由技巧和策略组成的。他们的策略可能是聪明的，但他们的本质仍然是伪造，模仿智慧。人工智能系统无法模仿现实或人类智能。他们的数据是部分的和不完整的，他们的程序是静态的和重复的。总的来说，他们整合的概念和行为是静态的、重复的、部分的和不完整的。人工智能系统可以满足大多数情况下的需要，但不足以解决它们没有能力解决的情况。因此，我们可以改写乔治·博克斯的格言:*所有的人工智能系统都是错误的，但有些是有用的。他们要错到什么程度才会没用？就目前而言，人工智能系统就像魔像一样:顺从，就像它们的人类创造者给它们的指令一样正确或错误。*

我们，人工智能系统的用户，应该开发我们对人工智能局限性的智能。我们需要理解人工智能的技巧和模仿。**我们需要人工智能来明智地使用人工智能**。也许到那时，在开发出我们自己的人工智能之后，我们将能够在我们的人造机器中编码它的基本原理。也许到那时，人工智能系统可以获得它们自己的智能。这可以赋予人工智能系统一种自我意识。

人工智能系统什么时候缺乏编码正确信息的变量？人工智能系统什么时候缺乏智能行动的指令(规则或代码行)？这些变量和指令应该是什么？首先，这些是我们开发人工智能需要问的基本问题。有一天，人工智能系统可以自己进行这样的分析。然而，对于人工智能系统来说，获取新的变量和指令，并有意义地应用它们仍然是非常具有挑战性的。相反，这个过程对人类来说更自然:我们的心理模型是灵活的，整合了对未知的认知。

毕竟，人类甚至认为智力本身是未知的。没有普遍的智力定义，也没有普遍的智力测试。这也许是由于智力的多元性。智力缺乏定义反映了我们对世界理解的灵活性。通过让我们的精神概念部分不确定，我们允许对世界更准确理解的空间出现。我们可以把这个过程看作是*【正当信念】*的建构，一个认识论的核心关注点，一个开发智力的实践框架。

奥卡姆的威廉推荐*“没有必要，就不要假定多数”*[【7】](https://en.wikipedia.org/wiki/Occam%27s_razor)。也许，对于发展智力来说，多元化是必要的。智能可能依赖于包含多个模型，这些模型可能与特定情况相关，但并不与所有情况相关。

![](img/71c0f6a1789815def601a5f2200ce434.png)

Photo: E. Beauxis-Aussalet

[1]https://en . Wikipedia . org/wiki/il marinen # il marinen ' s _ Bride _ of _ Gold
【2】[https://en.wikipedia.org/wiki/Hephaestus](https://en.wikipedia.org/wiki/Hephaestus)
【3】[https://en.wikipedia.org/wiki/Talos](https://en.wikipedia.org/wiki/Talos)
【4】[https://en.wikipedia.org/wiki/Golem](https://en.wikipedia.org/wiki/Golem)
【5】[https://en.wikipedia.org/wiki/Allegory_of_the_Cave](https://en.wikipedia.org/wiki/Allegory_of_the_Cave)
【6】[https://en.wikipedia.org/wiki/The_Treachery_of_Images](https://en.wikipedia.org/wiki/The_Treachery_of_Images)
【7】[https://en.wikipedia.org/wiki/Occam%27s_razor](https://en.wikipedia.org/wiki/Occam%27s_razor)
德雷珀，N. R. (1987)，经验模型建立和响应面，[约翰威利&儿子](https://en.wikipedia.org/wiki/John_Wiley_%26_Sons)。
[9]艾伦·贝克(2010) [2004]。[【简约】](http://plato.stanford.edu/entries/simplicity/)。*斯坦福哲学百科*。加州:斯坦福大学。[ISSN](https://en.wikipedia.org/wiki/International_Standard_Serial_Number)1095–5054。