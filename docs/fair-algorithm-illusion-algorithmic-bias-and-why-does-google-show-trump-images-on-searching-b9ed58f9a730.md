# 公平算法幻觉:算法偏见和为什么谷歌在搜索“白痴”时显示特朗普的图像

> 原文：<https://medium.datadriveninvestor.com/fair-algorithm-illusion-algorithmic-bias-and-why-does-google-show-trump-images-on-searching-b9ed58f9a730?source=collection_archive---------6----------------------->

![](img/7e9b65e918d15f3d0ba58ac88cf062a3.png)

## 谷歌搜索后台(联想傻逼==特朗普总统)

在最近的谷歌国会听证会上，参议员们质问首席执行官桑德尔·皮帅，为什么谷歌在搜索“白痴”一词时会显示川普的图片，并声称员工与结果有关。皮查伊先生很好地解释了搜索的内部工作原理(也就是 200 种不同的排列页面的信号),而且搜索是自动进行的，没有人工干预。对于一个不完美的问题来说，他的回答非常准确。一个更好的问题应该是:“谷歌算法把特朗普和一个单词搜索联系起来是不是有偏见，就像“白痴”一样？".当搜索“smart”或“intelligent”时，谷歌搜索会显示汽车和其他图片的列表。一致的算法会显示聪明人(例如爱因斯坦)的图像。谷歌的算法似乎受 200 个信号中的几个信号(例如，流行度、关联度、突出度、新鲜度)的影响，并将这些信号显示为从数亿搜索结果中选择的 rop 结果。上述内容并不意味着谷歌员工有什么邪恶的行为，而是说明了一个事实，即该算法为人类观察者提供了不一致的结果，从而产生了偏见。

## 算法偏差和客观人工智能模型的幻觉

我们期望算法是公平的，就像期望人类没有偏见一样，这是错误的。现有的大量研究表明，算法可能会由于多种原因而产生有偏差的结果。这些偏见的一些精选例子:

1.  屏幕科学:偏向算法的一个简单例子是应该在前几页或第一组条目上显示什么的相对优先级。甚至在人工智能出现之前，人们就已经知道并实现了第一个偏见。当第一个航空票务平台(由美国航空公司开发的 SABRE)推出时，它倾向于在搜索结果的开头显示美国航班，这使美国航空公司比其他航空公司更有优势。(美国航空公司因在 SABRE 的偏见而受到反垄断诉讼)。在网站和对话平台上，我们面临着相似的有偏见的结果。
2.  训练数据:用于模型的训练数据对模型输出有重大影响。例如，使用网站数据的单词嵌入模型开始创建一些关联，如男人-女人=计算机程序员-家庭主妇或与“她”最接近的职业，如行政助理、护士、家庭主妇。有很多这样的例子可以说明人类在数据集中的偏见是如何渗透到计算机模型中的。
3.  模型优化方法:数学模型和建模者在选择用于预测结果的特征时所做的选择，或者通过聚集特征来简化模型的行为会导致刻板偏见，就像人类一样。例如，许多模型使用“喜欢”来确定某人对什么感兴趣。这使模型偏向于任何可能产生兴趣的东西，即使它实际上可能是“假新闻”或一张鸡蛋的照片。推动美国和英国退出欧盟公投运动的建模者有效地使用了这些方法来推动他们的议程。

## 不公平算法/模型的商业含义

作为人类，我们现在才开始意识到我们偏见的后果，这是基于我们大脑中嵌入的神经模型，经过了一千多年的进化。人工智能模型目前正被用来帮助对有可能犯罪的人、大学入学、获得贷款的能力、LinkedIn 上的工作列表、购物清单、新闻进行评分。随着人工智能模型变得越来越普遍，带有偏见的模型的风险是真实而高的。一旦偏见嵌入到人工智能模型中，即使是用户和建模者也不可能解释特定场景的结果，更不用说证明其公平性了。“系统是这么说的，因此它肯定是对的”是人类相当普遍的反应。

尽管在当前科学发展的状态下，提供完全无偏见的结果几乎是不可能的，但是模型给公司声誉造成的潜在损害可能是巨大的。防范潜在的国会听证会/诉讼或改善客户体验和推动社会议程需要 1)量化模型和结果中的隐性或显性偏见，2)彻底评估和理解竞争模型选择的业务收益和风险，而不是让数据科学家和建模者自行决定，3)采取适当的预防措施来减轻任何风险，同时 4)改善模型背后的科学。公司需要主动考虑这些因素，以避免未来重大的业务和声誉风险。