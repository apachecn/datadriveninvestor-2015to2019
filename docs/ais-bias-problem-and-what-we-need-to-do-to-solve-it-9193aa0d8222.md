# 人工智能的偏见问题——以及我们需要做些什么来解决它

> 原文：<https://medium.datadriveninvestor.com/ais-bias-problem-and-what-we-need-to-do-to-solve-it-9193aa0d8222?source=collection_archive---------10----------------------->

[![](img/ed89380d4878d48e27165fde1cec8335.png)](http://www.track.datadriveninvestor.com/1B9E)![](img/c92fd5e36ed58cc23bd6aeab9b702839.png)

Photo courtesy: CNET

如今，人工智能(AI)已经普及到生活的各个角落。以亚马逊 Alexa 为例。我们甚至不会对有人要求 Alexa 播放一首歌，欢呼一个优步，或拉一张购物清单眨一下眼睛。人工智能在商业领域也变得越来越普遍。几乎每个人都看过关于网飞的电影或电视节目。那些建议的标题？这是网飞使用人工智能来评估过去的观看行为，以推荐新内容来保持你的参与和订阅。

人工智能为亚马逊、网飞和几乎任何现代公司带来了巨大的价值，有助于从数据中释放新的洞察力，并指导重大决策。但是，即使人工智能尽了一切可能，还是有一些事情需要注意——最重要的是:无意识的偏见。我们需要认识到潜在的偏见来自哪里，以防止问题在我们的人工智能应用程序中突然出现，并确保它们提供预期的结果。

**人工智能偏差的两大原因:数据和训练**

人工智能偏见的最大原因之一是数据样本缺乏多样性。简单地说，一个算法的好坏取决于你输入的数据。例如，航空公司将通过人工智能算法定期运行飞机引擎的传感器数据，以预测所需的维护。但是，如果一个系统用来自北美寒冷的上半部分的飞行数据进行训练，然后应用于中美洲和南美洲的飞行，数据可能会超出模型的参数，并提供错误的建议。

老实说，很难获得完整、全面的数据集来训练人工智能系统，所以大多数人只是使用容易和容易获得的东西。有时，我们甚至可能根本没有数据。例如，帮助人力资源团队选择多样化候选人的软件，如果只有同类员工的数据，可能会遇到困难。

偏见的另一个重要驱动因素发生在我们匆忙或不完整的训练算法时。例如，通过对话变得更加个性化的聊天机器人可能会使用不良语言，除非我们花时间训练算法不使用不良语言。微软通过其 Twitter 机器人“Tay”学到了这一点，它开始发布一些非常政治错误的推文。对适当算法训练的担忧是为什么很难证明在刑事司法这样重要的领域使用人工智能是正确的。

为什么要进行一些速成训练？你可以说这是由于敏捷编程的流行，它促进了短期迭代开发。最重要的是，围绕人工智能的兴奋导致了过早的应用。许多人没有花时间和精力在规划和设计工作上，特别是当涉及到当前人工智能在处理常识、公平和公正等方面的限制时。这就是拥有领域知识的人的关键所在。领域专家可以帮助思考潜在的偏见，相应地训练模型，并管理机器以确保它们不会出错。

**数据和团队的多样性有助于**

防止人工智能偏见的最佳方式是使用全面的数据集，考虑所有可能的用例。如果有不平衡的数据，我们可以从外部来源来填补空白，给系统一个更完整的描述。一言以蔽之，你的数据越全面，人工智能就越准确。

与人工智能合作的团队的多样性也可以解决培训偏见。当我们只有少数几个人在一个系统上工作时，它就会偏向一小群人的想法。通过引入一个拥有不同技能和方法的团队，我们可以有一个更全面、更道德的设计，并提出新的角度。

以一家财富管理公司为例，该公司有多个团队训练一种算法，以提高交易收入。显而易见的方法是查看 30-35 岁的单身男性日内交易者的数据，这是预期的最佳时机。其中一个团队——成员包括客户体验和领域专家——不仅实现了最初的目标，还在 50-55 岁的单身女性群体中发现了另一个机会——这是一个以前未开发的高投资细分市场。如此多样化的团队发现了我们不知道该问的问题。

**人工智能也能最小化偏差**

对于所有关于人工智能偏见的说法，我们实际上正在使用人工智能来解决人类决策和行动中的无意识偏见。回到招聘的例子，工作描述可能有无意识的性别偏见，这可能会阻止男性或女性申请一个职位。一个人工智能程序可以被训练来审查描述，标记和替换可以被解释为超男性化或女性化的单词，例如使用“作战室”，并用“神经中枢”使其更加中性。

在处理人工智能中的无意偏见时，有几件关键的事情要记住。需要有全面的数据覆盖，以确保我们可以训练机器在所有场景下工作。拥有领域知识的多元化团队可以提供帮助，带来不同的角度和想法。但是我们不能操之过急。我们需要正确的设计、规划和治理，以确保该技术兑现其承诺。