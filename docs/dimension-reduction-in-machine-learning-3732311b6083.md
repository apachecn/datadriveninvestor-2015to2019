# 机器学习中的降维

> 原文：<https://medium.datadriveninvestor.com/dimension-reduction-in-machine-learning-3732311b6083?source=collection_archive---------1----------------------->

我本科最后的研究是为一个旅游领域创建一个基于本体的问答系统。为此，我们使用问题分类方法，从网上收集与旅行相关的问题，并根据预期的答案类型标记数据。使用分层分类法标记预期的答案类型。该项目的更多细节可以在[这里](https://ieeexplore.ieee.org/document/7980526/)找到。以下是我们在问题分类器中使用的一些功能:

*   一克
*   双克
*   中心词的同义词

正如你所看到的，上述所有特征都作为稀疏位图馈入机器学习模型，这将增加维度的诅咒。这既会增加机器学习算法的训练时间，又会降低最终的准确率。为了解决这个问题，我们对降维方法进行了一些研究。降维方法用于减少训练数据特征的维数。本文将包括一些流行的降维方法。

![](img/b23c5519fed066b46e49f81d4ac28362.png)

Photo by [Aditya Chinchure](https://unsplash.com/@adityachinchure?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

**缺失值**

在解决机器学习和数据挖掘问题时，一个常见的问题是我们会发现具有缺失数据的特征或列。为了处理这些丢失的数据，我们可以删除列，填充丢失的值，或者删除整个特性。如果特性丢失了一半以上的数据，并且维度的诅咒很高，那么最好的方法是删除丢失数据的列。

在我们的问题分类器中，我们对每个中心词使用词网同义词。Word nets 支持每个中心词多达 3 个同义词，但大多数中心词都缺少第三个同义词，因此我们不得不放弃一些功能的第三个同义词功能，这最终会降低数据的维度。

**低方差**

在一些问题中，我们遇到了具有低方差的特征，其中超过 80%的数据点对于特定的特征具有相同的值。此时，由于这些类型的特征对最终的准确性没有贡献，我们放弃了这些特征。在英语中，停用词经常出现。bi grams 中的一些停用词列对最终的准确性没有贡献，所以我们从 uni grams 的稀疏位图中删除了停用词列。

**决策树和随机森林**

决策树和随机森林是两种流行的监督学习算法。决策树和随机森林的基础是熵和信息增益。这些算法使用具有最大信息量的特征进行学习。因此，当给定一个问题时，我们可以使用这些算法来识别具有高信息增益的特征。

![](img/05677312f553a72b0dc1c9608d3e6d46.png)

Photo by [Thomas Lambert](https://unsplash.com/@lmbrtt?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

**高相关性**

彼此相关性较高的特征意味着这两个特征包含相同的信息。作为一个例子，我们可以用出生日期和年龄。大多数机器学习算法都假设提供给它们的特征不是多重共线性数据。因此，我们不得不丢弃相关性高的特征。为此，我们可以使用相关矩阵。在自然语言中，人们在交流时使用简短的形式(LA 代表洛杉矶等。).因此，为了解决这个问题，我们将相关性高的列合并为一列。

上述方法的主要缺点是特征的选择最终需要人工监控。这可以通过主成分分析(PCA)来处理。

**主成分分析**

主成分分析(PCA)是一种统计程序，它使用正交变换来转换一组可能相关变量的观察值。PCA 是机器学习中用于降维的统计方法。PCA 将一组高维数据转换成一组新的维度。人们主要使用这种方法来可视化高维数据。在我们的项目中，我们使用层次分类法作为数据集的标签，因此在设计分类法时，我们在可视化数据方面遇到了巨大的问题。因此 PCA 被用来将高维稀疏数据转换到三维数据空间。

pca 作为一种降维方法的主要缺点是它很难解释，因此它不能很好地处理非线性数据。如果你想对非线性数据进行降维，你可以使用 T-SNE 或者拉普拉斯特征映射**。**

[![](img/77a7e9c7cd800c68bee06b751e8aed70.png)](http://eepurl.com/dw5NFP)