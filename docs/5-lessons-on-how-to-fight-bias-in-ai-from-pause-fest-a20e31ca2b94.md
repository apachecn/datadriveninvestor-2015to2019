# 从 Pause Fest 学到如何对抗人工智能偏见的 5 课

> 原文：<https://medium.datadriveninvestor.com/5-lessons-on-how-to-fight-bias-in-ai-from-pause-fest-a20e31ca2b94?source=collection_archive---------12----------------------->

[![](img/c67bb6244e891913febbc6f9ea9f1451.png)](http://www.track.datadriveninvestor.com/1B9E)

## 从谷歌、微软和最好的科技公司吸取的教训

![](img/e273dcab00e91d60a7bfa783b6b037bc.png)

上周，我应战略 IT 咨询公司 KJR 的邀请，在他们的小组会议上就“风险是真实的:揭露人工智能中的道德和偏见”发表演讲，加入了来自人工智能健康和 KJR 的其他人工智能专家。

科技领域的偏见、道德和多样性似乎是 Pause Fest 上的一个大话题，几场会议涵盖了这些领域，包括来自谷歌和微软的会议。

这里有 5 个教训，从小组和暂停节一般。

# 1.使用工具检测算法偏差

IBM 已经开发了一个实时算法偏差检查器，可以运行在任何算法上，分析算法如何以及为什么做出决策。公平 360 套件也将扫描偏差的迹象，并建议调整。

谷歌推出了“假设工具”，帮助用户了解他们的机器学习算法是如何工作的。

微软还在 2018 年初宣布，它将推出自己的偏见检测工具包，除非它被嵌入到它已经提供的人工智能服务中，否则它似乎还不可用。

KJR 的首席顾问 Karolyn Gainfort 谈到了谷歌举办的一个研讨会，他们在研讨会上讨论了如何为无人驾驶汽车建立道德规则。在研讨会结束时，解决方案架构师说这是无用的，因为人类司机不会根据你的性别和年龄等特征对一个人的生命价值进行复杂的计算。你只要猛踩刹车，希望你不会撞死任何人。

> 当有疑问时，建立在人类的直觉上，人类不会过着基于在路上杀谁的复杂规则的生活。

# 2.采用人工智能道德框架

检查偏见和道德的代码审查应该基于道德框架。你需要知道你在检查什么。

在小组讨论会上，我提到了微软指导人工智能开发和使用的六项伦理原则:

*   公平——人工智能系统应该公平对待所有人
*   可靠性和安全性——人工智能系统应该安全可靠地运行
*   隐私和安全——人工智能系统应该是安全的，并且尊重隐私
*   包容性——人工智能系统应该赋予每个人权力，让人们参与进来
*   透明度——人工智能系统应该是可以理解的
*   责任——人工智能系统应该有算法责任

这类似于谷歌根据这些目标评估其人工智能应用的方式，并认为它应该:

*   对社会有益
*   避免制造或强化不公平的偏见
*   为了安全而制造和测试
*   对人们负责
*   融入隐私设计原则
*   坚持科学卓越的高标准
*   可用于符合这些原则的用途

> 这些原则应该是开发你自己的人工智能伦理框架的良好基础。

# **3。强制代码和解决方案评审**

AI Health 的创始人科迪·米德尔布鲁克(Cody Middlebrook)提到了他在微软的时候，当时采取了许多措施来限制糟糕的功能或产品被发布的机会。

微软有一个过程，在这个过程中，工程师不能将代码签入源代码控制，除非代码由不属于项目的独立人员审查。

也有一系列的自动化测试，测试变量和公差将被纳入该功能。

对于每个新功能或项目，工程师还需要提出三种解决方案，例如三种设计模式，工程师必须解释所选方案背后的基本原理，然后对三种解决方案进行独立评估。

> 开发一个健壮的代码审查过程，并包括单元测试，以检查输出是否是您对模型的期望。

# 4.为团队多元化制定目标

谷歌 AR/VR 首席 UX 工程师萨姆·基恩(Sam Keene)发表了关于创建世界规模的 AR 的演讲。这次谈话发生在谷歌宣布他们新的谷歌地图 AR 产品的前一天，该产品只向少数用户发布。你是幸运儿之一吗？

Sam 提到，团队中几乎每个人都是跨学科的，都参与了研究、开发和设计。他还提到，多元化是他在团队中努力追求的目标。

对一个团队中好的人员组合有一个清晰的概念，会使正确招聘变得更容易。制定性别比例、教育背景甚至文化背景的目标会有所帮助。公司不能再为单一的市场设计，因此拥有一个多样化的团队是有意义的。

> 尽管人们对择优录用存在争议，但你不必妥协。你只需要知道你想让谁加入你的团队，并确保你能接触到这些人。

# 5.允许用户轻松提供反馈

谷歌董事兼首席 UX 研究员里卡多·普拉达谈到了谷歌如何通过在服务中嵌入报告工具来允许用户报告他们提供的服务中的错误或不当内容。

在谷歌使用机器学习的一些最先进的技术中，他们一直在努力防止这种技术延续人类偏见。这包括从搜索结果顶部删除攻击性或误导性信息，并在搜索栏中添加反馈工具，以便人们可以标记不适当的自动完成搜索结果。

这里有一个视频，更多地解释了谷歌如何解决机器学习中的人类偏见。

Source: Google

**我写的是艾与**[](https://www.transhumanism.com.au/)****。如果你也想了解一个受新兴技术影响的世界，请跟随我。****

****本文原载于** [**数据驱动投资人**](https://www.datadriveninvestor.com/2019/02/18/5-lessons-on-how-to-fight-bias-in-ai-from-pause-fest/) **。****