# 人工智能中的偏见不是人为的

> 原文：<https://medium.datadriveninvestor.com/bias-in-a-i-isnt-artificial-5350c3940dac?source=collection_archive---------11----------------------->

## 因为我们思考的方式塑造了我们建造的一切

![](img/09d2229fa6b5fbf2483c9e90140cfa66.png)

> 人工智能中的算法偏见不是那么人工的，它实际上是难以置信的有机的。—盖拉·博斯科维奇

全世界的人和组织每天都在使用人工智能和机器学习。我们用它来确定互动、分类、机会，并推动决策。我们在任何地方都使用它；在工作场所、家庭、企业和部分刑事司法系统中。

然而，工业界和学术界都越来越多地发现人工智能中使用的算法的偏见。这些包括种族歧视、性别歧视、阶级歧视和地域歧视。然而，为了创新和进步，我们继续依赖这些算法并创造新的算法。这些类型的算法的应用变得越来越普遍——在执法中，确定某人是否是犯罪活动的“高风险”,雇佣谁工作，谁应该获得贷款，以及许多行业要做出的数百个其他决定。

[](https://www.datadriveninvestor.com/2019/03/03/editors-pick-5-machine-learning-books/) [## DDI 编辑推荐:5 本让你从新手变成专家的机器学习书籍|数据驱动…

### 机器学习行业的蓬勃发展重新引起了人们对人工智能的兴趣

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2019/03/03/editors-pick-5-machine-learning-books/) 

从根本上说，技术应该是中性的。因此，问题在于，我们如何挑战人工智能/机器学习的冰冷数学？

在宏观层面上，机器学习算法建立在假设和反馈数据的基础上。

但是有一些细微的差别在起作用。首先，算法本身存在问题——假设——然后，我们提供给这些假设的数据也存在问题。

假设是我们在现实世界中开始处理信息的方式。我们假设轶事或经验证据将是人和事物行为方式的准确表现。这就是为什么我们会有刻板印象，这也是为什么这些刻板印象很少成立的原因。我们都知道他们是怎么说假设的…

另一方面，我们正在处理两种不同类型的数据，结构化数据和非结构化数据。结构化数据是精选的数据；它通常由人进行分类和组织。这就像 Excel 电子表格中的字段。另一类是非结构化数据，它还没有被管理。它只是被混在一起，没有分类，而且相当混乱。

现在，当涉及到偏见时，结构化数据有很高的被偏见的风险，因为循环中的人会对数据的特定属性或特征的重要性做出假设。我的朋友兼思想领袖盖拉·博斯科维奇说得好，“人工智能中的算法偏见不是那么人工的，它实际上是令人难以置信的有机的。”这就是风险发生的原因，因为它是有机的；不一定是因为偏见是恶意的或有目的的。其根本原因是人类的本性和我们关于我们如何认为某些数据属性或某些情报是重要的主观性。

以面部识别软件为例。

面部识别软件越来越多地被用于执法目的。2018 年 2 月，麻省理工学院的一名研究人员发现，三家公司的性别识别人工智能软件可以在 99%的情况下从照片中正确识别一个人的性别，但仅限于白人男性。对于深色皮肤的女性，准确率下降到只有 35%。

公司发现甚至很难获得足够的多样化图像输入到系统中。这本身就很可怕！这表明了现实世界中的一些偏见如何渗透到人工智能中，从而影响面部识别。

当然，偏见不仅限于面部识别和图像。另一个例子是语音转文本服务，它输入音频并输出转录。某些口音和方言比其他口音和方言更容易翻译，错误更少。想象一下人工智能被用于刑事判决(一种使用关于被告的数据来衡量他或她未来犯罪的可能性的算法)，它的决定基于一些因素，包括某人是否住在政府补贴的住房中，或者是否表达了“对警察的负面态度”。(对于任何对这个话题感兴趣的人，大西洋月刊今年夏天发表了一篇精彩的文章，名为德里克·汤普森的《[我们应该害怕刑事司法系统中的人工智能吗？)](https://www.theatlantic.com/ideas/archive/2019/06/should-we-be-afraid-of-ai-in-the-criminal-justice-system/592084/)

如果我们继续发展和依赖人工智能(我们现在是，我们也会是——所以这是一个修辞问题)，我们知道这些问题应该得到解决。然而，我们看到大公司的人工智能软件没有经过预先的偏见测试就上市了。它也没有被测试，因为那部分不是优先考虑的，因为，好吧，让我们面对它，很有可能做建筑的团队不想在投入时间和资源后举手说，“对不起，老板，我们的算法有点种族歧视。而且好像也不怎么喜欢女人……”但是，我猜的。算是吧。

有解决办法吗？当然，对于我们所知道的问题，确实有改进和解决的机会。

首先，工业界应该更加努力地使构建模型的团队多样化。如果我们关注种族、性别和背景的多样化，那么我们就能确保这些团队从各个角度思考体验，而不仅仅是他们觉得舒服的那些。

然后，我们需要通过查看我们已经拥有的和正在收集的数据来解决数据问题。我们应该强迫自己问我们都在想的问题，但是没有人开口。为什么我们没有足够的数据？这为谁工作？(如果答案不是每个人，我们需要解决这个问题。)我们怎样才能做得更好？

因为我们思考的方式塑造了我们建造的一切。我们必须开始以不同的方式思考，这样我们才能以不同的方式开始建设。建造得更好。

关于这个话题的更多信息，请查看我在 Reg 上的最新一集，我在伦敦采访了特邀嘉宾 Ghela Boskovich。盖拉是思想领袖、多样性倡导者、FemTechGlobal 和全能坏蛋的创始人。

达拉·塔尔科夫斯基。企业家、作家、演说家、母亲|打造新事物；联合创始人[@ actuate law](https://actuatelaw.com/)\[@ quointec](http://quointec.com/)