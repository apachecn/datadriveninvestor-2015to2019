# 为什么我们需要关心人工智能的敏感性

> 原文：<https://medium.datadriveninvestor.com/why-we-need-to-care-about-ai-sensitivity-a6ae0353ff65?source=collection_archive---------4----------------------->

# 技术反映，而不是抹去，我们的偏见。

![](img/f383d9d0b2af8fb327d78917aab04058.png)

这是一个永远值得重复的事实:计算机只知道我们编程让它们知道的东西。这一基本事实使得一种错误的想法成为可能，即价值中立的代码语言从等式中抹去了所有过于人性化的偏见。毕竟，我们认为人类是容易犯错的思考者，容易做出不公平的判断和有偏见的决策。理论上，机器应该更好，不是吗？

到现在为止，我们已经习惯于听这个(错误的)故事了。出色的软件正在彻底改变我们的世界，使它变得更加公平和丰富多彩。然而，不幸的是，许多人认为人工智能的决策不容易受到偏见的影响，这显然是错误的。虽然一个程序可以评估纯数据而不像人类那样做出快速判断，但有问题的信息通常是基于过去和现在的偏见决定，在现实世界中，歧视是边缘化人群的一个持续现实。在一个大量投资于其改变世界形象的行业中，歧视被我们的许多“革命性”技术工具维持，而不是消除，这变得非常明显。

[](https://www.datadriveninvestor.com/2019/02/19/artificial-intelligence-trends-to-watch-this-year/) [## 今年值得关注的 5 大人工智能趋势|数据驱动的投资者

### 预计 2019 年人工智能将取得广泛的重大进展。从谷歌搜索到处理复杂的工作，如…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2019/02/19/artificial-intelligence-trends-to-watch-this-year/) 

本质上，算法是一组程序指令。对于一个人来说，理解难以理解的庞大数据集是非常耗时的，所以我们把它留给这些预先制作的程序来处理软件中最神奇的工作。我们今天使用的大多数创新软件服务都依赖于人工智能——从网飞建议到智能手机中的自动更正。这种解决问题的方法是当今技术所固有的。

重要的是，仔细看看这些奇迹是如何发生的。正是由于许多行代码(通常有几百万[到几百万](https://informationisbeautiful.net/visualizations/million-lines-of-code/))的存在，即使是最基本的软件应用程序也能正常工作，而人工智能和机器学习的世界正是在这些难以理解的数据行中的算法中形成的。

**遗留的不平等**

有太多“智能”软件未能给予边缘群体用户同等信任的例子。一种主要在白人面孔上测试的面部识别算法[在识别黑人面孔](https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html)时会出现问题。一个受过识别人类语言训练的聪明助手无法正确地将[与女性的声音](https://www.theregister.co.uk/2018/03/14/voice_recognition_systems_are_naturally_sexist/)协调起来。这些不应该被解读为仅仅是需要用软件补丁来纠正的怪癖，而是呼吁科技行业好好审视一下他们的做法。

其后果比消费技术领域的错误识别要严重得多。[监狱判刑](https://www.npr.org/2017/11/25/566438860/research-finds-racial-disparities-in-prison-sentences)、[房屋贷款分配](https://www.nytimes.com/2017/08/24/upshot/how-redlinings-racist-effects-lasted-for-decades.html)、[汽车保险费率](https://www.propublica.org/article/minority-neighborhoods-higher-car-insurance-premiums-white-areas-same-risk)等等历史上已经显示出歧视性的做法，许多这样的决定现在都交给了 AI。随着算法在政府和私人的这些过程中发挥更大的作用，很明显，在高科技移交后，昨天的[偏见和今天的](https://www.aclu.org/blog/racial-justice/race-and-economic-justice/big-data-could-set-insurance-premiums-minorities-could)[偏见没有丝毫减缓的迹象。随着创造它的科技公司表现出对纠正这种不公正不感兴趣，修复算法偏见造成的错误就留给了其他强大的玩家。](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

**从以上变化**

就像科技责任的其他热点问题一样，我们大西洋彼岸的邻国在这方面正在发挥积极的带头作用。T2、法国和 T4、英国的政府领导人都特别直言不讳地表示，需要确保引导我们生活的技术更加公平。来自高层的这类信件带来了充满希望的未来，但仍有许多工作要做。

在美国，我的家乡纽约市发起了对城市商业中算法使用情况的全面审计，这是美国第一个这样的项目。自动决策系统任务组成立于 2018 年 5 月，将于今年年底在一份完整的报告中提交他们的研究结果。该组织由科学家、教授、倡导者和其他专家组成，很可能会叫停许多众所周知具有歧视性的算法流程。

**为未来而战**

虽然这可能被解释为太少，太晚了，但事实是，尽管已经取得了所有的进展，但迄今为止，技术巨头们只触及了人工智能潜力的表面。“智能”技术的使用将继续增长，所以现在就采取行动是绝对重要的。已经造成的损害是无法否认的，但通过有意识的行动，我们可以防止未来的边缘化人群因算法驱动的消费者和政府应用的扩展而受到更多伤害。

纽约的项目不仅有望成为其他城市的行动号召，也有望让程序员和他们的雇主意识到智能算法的局限性。增加科技行业多样性的主要举措也有望从根本上打击歧视，让更加多样化的劳动力直接了解继续困扰科技世界及其周围更大社会的历史性偏见。随着技术本身的发展，没有理由相信创造技术的行业不能同步进步。

***原载于*** [***发迹全球***](https://thriveglobal.com/stories/why-we-need-to-care-about-ai-sensitivity/)

![](img/5cc1f17de57213a170a72871197a712c.png)

*Bennat Berger 是纽约的一名企业家、投资者和科技作家。他是 Novel Property Ventures 的联合创始人和负责人，这是一家房地产公司，专门从事纽约市多户住宅单元的积累和管理。他还是投资公司 Novel Private Equity 的创始合伙人，负责监管各种兴趣领域的投资，从体验式零售到娱乐再到超市技术。*