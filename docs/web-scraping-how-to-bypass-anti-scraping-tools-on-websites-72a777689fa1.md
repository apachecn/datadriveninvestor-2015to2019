# 网页抓取:如何绕过网站上的反抓取工具

> 原文：<https://medium.datadriveninvestor.com/web-scraping-how-to-bypass-anti-scraping-tools-on-websites-72a777689fa1?source=collection_archive---------7----------------------->

![](img/f11d62c08730c508bdfaae88911557f2.png)

Photo by [Luca Bravo](https://unsplash.com/@lucabravo?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

这是一个竞争激烈的时代；企业为了出人头地，用尽一切办法。对于商家来说，实现这种至高无上的唯一工具就是 ***网页抓取*** 。但是这也不是一个没有障碍的领域。网站利用各种 ***防刮技术*** 从他们的网站上攻击你。但是总会有办法的。

## 我们对网络抓取了解多少

WWW 包含的网站比你想象的要多。其中一些可能与您的域名相同。比如[亚马逊](https://blog.datahut.co/challenges-that-make-amazon-data-scraping-so-painful/)和 Flipkart 都是 ***电商网站*** 。这样的网站会成为你的对手，甚至不用尝试。因此，当谈到品尝成功时，你需要确定你的竞争对手并征服他们。

那么，有什么方法可以帮助你在同一个领域超过一百万人呢？

答案是 [***网刮***](https://blog.datahut.co/choosing-a-web-scraping-service-think-about-the-total-cost-of-data-ownership/) 。

***网页抓取*** 无非是从各种网站收集数据。你可以[提取信息](https://blog.datahut.co/price-comparison-on-amazon-how-web-scraping-helps-companies-win-the-e-commerce-game/)，比如产品定价和折扣。您获得的 ***数据*** 可以帮助提升用户 [***体验***](https://blog.datahut.co/how-brands-use-data-to-enhance-the-consumer-experience/) 。反过来，这种用法将确保客户更喜欢你而不是你的竞争对手。

[](https://www.datadriveninvestor.com/2019/02/21/best-coding-languages-to-learn-in-2019/) [## 2019 年最值得学习的编码语言|数据驱动的投资者

### 在我读大学的那几年，我跳过了很多次夜游去学习 Java，希望有一天它能帮助我在…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2019/02/21/best-coding-languages-to-learn-in-2019/) 

比如，你的 ***电子商务*** 公司卖软件。你需要了解如何改进你的产品。为此，你必须访问销售软件的网站，了解他们的产品。一旦你做到这一点，你也可以核实你的竞争对手的成本。最终，你可以决定你的软件的价格以及哪些功能需要改进。这个过程几乎适用于任何产品。

# 什么是防刮擦工具，如何处理

作为一个成长中的企业，你将不得不瞄准流行和成熟的网站。但是 ***网页抓取*** 的任务在这样的情况下就变得困难了。为什么？因为这些网站采用各种 ***防刮技术*** 来阻挡你的去路。

这些 ***防刮工具*** 是做什么的？

网站包含大量信息。真正的访问者可能会利用这些信息来学习一些东西或者选择他们想要购买的产品。但是像竞争对手网站这样的非正版访问者可以利用这些信息在游戏中取得领先。

任何人都想阻止竞争。这就是网站使用 ***防刮工具*** 的原因。这些工具可以识别非正版访客，防止您获取 [***数据*** 供您](https://blog.datahut.co/how-brands-turn-data-into-profits-and-you-can-too/)使用。

这些 ***防刮技术*** 可以简单到 IP 地址检测，也可以复杂到 Javascript 验证。让我们看看几种绕过这些 ***防刮擦工具*** 中最严格的方法。

## 1.不断轮换你的 IP 地址

这是欺骗任何 ***防刮工具*** 最简单的方法。一个 ***IP 地址*** 就像一个分配给设备的数字标识符。当你访问一个网站执行 ***网络抓取*** 时，你可以很容易地监控它。

大多数网站都在检查访问者用来上网的 ***IP 地址*** 。所以，在完成搜索大型站点的巨大任务时，你应该准备好几个 IP 地址。你可以把这想象成每次出门都使用不同的口罩。通过使用这些，你的 ***IP 地址*** 都不会被拦截。

这种方法在大多数网站上都很方便。但是少数高调的网站使用 ***高级代理黑名单*** 。这就是你需要表现得更聪明的地方。住宅或移动代理在这里是可靠的选择。如果你想知道，有几种代理。

我们在世界上有固定数量的 ***IP 地址*** 。然而，如果你设法拥有 100 个这样的网站，你可以轻松地访问 100 个网站而不会引起任何怀疑。所以，最关键的一步是找到合适的代理服务提供商。

## 2.使用真实的用户代理

用户代理是 HTTP 头的一种类型。它们的主要功能是破译你用哪个浏览器访问一个网站。如果你使用的是一个不重要的网站，他们很容易就能找到你。例如，一些重要的网站可以是 Chrome 和 Mozilla Firefox。大多数 ***刮刀*** 都忽略了这一点。你可以通过[设置一个看似真实且知名的](https://www.youtube.com/watch?v=u8Sy3c2K6VU)用户代理来降低你被 ***列入*** 黑名单的几率。

您可以从 ***用户代理*** 的[列表中轻松找到自己。如果你的是一个高级网站，谷歌机器人用户代理可以帮助你。你的请求将允许谷歌机器人通过你的网站。此外，这将把你列在谷歌上。](https://developers.whatismybrowser.com/useragents/explore/)

当用户代理是最新的时，它工作得最好。所有的浏览器都使用不同的 ***用户代理*** 。万一你没有及时更新，你会引起你不想要的怀疑。在几个用户代理之间轮换也能让你占上风。

## 3.在每个请求之间保持随机间隔

一个 ***网刮*** 就像一个机器人。 ***网页抓取*** 工具会每隔一段时间发送请求。你的目标应该是尽可能表现得像个人。既然人类不喜欢例行公事，那就最好以随机的时间间隔来间隔你的请求。这样你就可以轻松躲闪目标网站的任何 ***防刮工具*** 。

确保你的请求是礼貌的。万一你频繁发送请求，可以让所有人的网站崩溃。目标是在任何情况下都不要让站点超载。举个例子， [***刺儿头***](https://blog.datahut.co/tutorial-how-to-scrape-amazon-data-using-python-scrapy/)*有一个强制要求发出请求要慢。*

*作为附加保障，可以参考某网站的 [robots.txt。这些文档中有一行指定了爬行延迟。相应地，您可以了解需要等待多少秒才能避免产生高服务器流量。](https://en.wikipedia.org/wiki/Robots_exclusion_standard)*

## *4.推荐人总是有帮助的*

*一个 [***HTTP 请求头***](https://www.webopedia.com/TERM/H/HTTP_request_header.html) 指定你重定向的站点是一个头。在任何 ***网页抓取*** 操作中，这都可以成为你的救命稻草。你的目标应该是让自己看起来像是直接来自谷歌。*

*标题*

*" Referer ":"[https://www.google.com/](https://www.google.com/)"*

*可以帮你做到这一点。你甚至可以在更换国家时改变这一点。例如，在英国，你可以用“[https://www.google.co.uk/](https://www.google.com/)”。许多站点确定 ***referrers*** 来重定向流量。你可以使用类似[https://www.similarweb.com](https://www.similarweb.com/)这样的工具为一个网站找到常见的 ***推荐人*** 。*

*这些 ***推荐人*** 通常是像 Youtube 或 facebook 这样的社交媒体网站。*

*了解 ***推荐人*** 会让你显得更真实。目标网站会认为该网站通常的推荐人会将你重定向到他们的网站。所以目标网站会把你归类为正版访客，不会想到 ***屏蔽*** 你。*

## *5.避免任何蜜罐陷阱*

*随着机器人变得越来越聪明，网站管理人员也变得越来越聪明。许多网站都有隐形链接，你的抓取机器人会跟着链接。通过拦截这些机器人，网站可以轻松地 ***拦截*** 你的 ***网页*** ***刮*** 操作。*

*为了保护自己，请尝试在链接中查找“显示:无”或“可见性:隐藏”CSS 属性。如果您在链接中检测到这些属性，那么是时候回溯了。通过使用这种方法，网站可以识别和陷阱任何编程刮刀。他们可以对你的请求进行指纹识别，然后永久阻止它们。*

*这是 ***网络安全*** 的[高手用来对付 ***网络爬虫*** 的方法。尝试检查每一页是否有这样的属性。网站管理员也使用一些技巧，比如将链接的颜色改为背景的颜色。在这种情况下，为了增加安全性，请查找类似“color:#fff”或“color:#ffffff”的属性。这样，你甚至可以避免链接被隐藏。](https://blog.datahut.co/hiring-a-data-scientist-decoding-the-ambiguities/)*

## *6.更喜欢使用无头浏览器*

*如今，网站使用各种欺骗手段来验证访问者是否真实。例如，他们可以使用 ***浏览器******cookie、Javascript、扩展和字体*** 。执行 ***网页抓取*** 这些网站可能是一项繁琐的工作。在这种情况下，a 可以成为你的救命恩人。*

*有许多工具可以帮助您设计与真实用户使用的浏览器相同的浏览器。这一步将帮助您完全避免检测。这种方法的唯一里程碑是这类网站的设计，因为它需要更多的谨慎和时间。但结果是，当 ***抓取*** 一个网站时，这是最有效的不被发现的方法。*

*这种智能工具的缺点是它们的内存和 CPU 密集型属性。只有当你无法避免被网站列入黑名单时，才求助于这些工具。*

## *7.控制网站的变化*

*网站可以因为各种原因改变布局。大多数时候，网站这样做是为了阻止网站从 ***刮*** 他们。网站可以在随机的地方包含设计。这种方法甚至被大网站使用。*

*因此，您使用的应该能够很好地理解这些变化。你需要能够察觉到这些正在发生的变化，并继续执行 ***网页抓取*** 。监控 ***每次爬网的成功请求数*** 可以帮助您轻松做到这一点。*

*确保持续监控的另一种方法是为目标站点上的特定 URL 编写单元测试。您可以从网站的每个部分使用一个 URL。这个方法将帮助你发现任何这样的变化。每 24 小时仅发送几个请求将帮助您避免 ***刮擦*** 过程中的任何暂停。*

## *8.采用验证码解决服务*

*[***是应用最广泛的防刮工具。***](https://blog.datahut.co/web-scraping-non-programmers/) 很多时候，无法绕过网站上的验证码。但是作为一个隐居者，许多服务被设计来帮助你进行 ***网络搜集*** 。其中一些是验证码解决方案，如 ***AntiCAPTCHA*** 。*

*需要 ***验证码*** 的网站强制 ***爬虫*** 使用这些工具。其中一些服务可能会非常缓慢和昂贵。所以你必须做出明智的选择，确保这项服务对你来说不会太奢侈。*

## *9.谷歌缓存也可以是一个来源*

*纵观 WWW，有大量的 ***平稳数据*** (随时间变化不大的数据)。在这种情况下， ***谷歌缓存的*** 副本可以作为你进行*网络抓取的最后手段。通过使用[缓存副本](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781782164364/3)，可以直接执行 ***数据采集*** 。这种方法有时比 ***抓取网站*** 更容易。**

**只需在你的网址前加上“[http://webcache.googleusercontent.com/search?q=cache](http://webcache.googleusercontent.com/search?q=cache):”作为前缀，你就可以开始了。这个选项对于那些很难抓取的网站来说很方便，但是随着时间的推移，这些网站会保持不变。**

**这种选择基本上是没有麻烦的，因为没有人总是试图 ***阻止*** 你的方式。**

**但这不是一个可靠的选择。例如，LinkedIn 一直拒绝谷歌缓存他们的数据。最好是选择一些其他方法来刮那个网站。**

***原载于 2019 年 12 月 26 日*[*https://blog.datahut.co*](https://blog.datahut.co/web-scraping-how-to-bypass-anti-scraping-tools-on-websites/)*。***