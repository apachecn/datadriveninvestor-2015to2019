# 拥有更多特征会导致更好的预测吗？

> 原文：<https://medium.datadriveninvestor.com/having-more-features-leads-to-a-better-prediction-yes-no-f82cc6183b37?source=collection_archive---------0----------------------->

[![](img/70cc2b5531cf831a2348ede69e1386d5.png)](http://www.track.datadriveninvestor.com/1B9E)

***作者仅代表作者个人观点。这些绝不代表和/或反映他所关联的组织。***

![](img/97fdb7aac10104debd929ff3c07ab332.png)

**最初的想法**

数据是所有机器学习问题的关键，但需要多少类型的数据才能得到一个伟大的模型一直是有争议的。在典型的房屋预测示例中，通过将大小作为输入来预测房屋价格是可以的，但为了获得更准确的预测，我们可以考虑添加更多的特征，如楼层号、公寓的朝向/方向、卧室数量、便利设施数量，等等。

因此，随着我们不断添加越来越多的特征，预测的准确性将会提高，但是，我们可能需要有意识地呼吁在多大程度上必须这样做，因为每个额外的特征都会使计算变得昂贵，因为我们要处理大量的数据以及复杂的数学计算。

[](https://www.datadriveninvestor.com/2019/03/03/editors-pick-5-machine-learning-books/) [## DDI 编辑推荐:5 本让你从新手变成专家的机器学习书籍|数据驱动…

### 机器学习行业的蓬勃发展重新引起了人们对人工智能的兴趣

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2019/03/03/editors-pick-5-machine-learning-books/) 

我们把事情复杂化了吗？多少个这样的特征就足够了？如果给我们一个包含更多特征的大数据集，我们能做些什么呢？我们有办法优化它吗？这些可能是此时此刻萦绕在我们脑海中的一些问题。让我们开始探索细节，以揭开其中一些问题的答案。

**处理多重特性的潜在问题是什么？**

所以现在我们都相信拥有多个特征这一点，因为这将有助于提高预测的准确性。然而，当我们将这一想法付诸实施时，真正的挑战就来了，因为这些不同的特征可能无法在同一尺度上量化。其中一些要素的值可能不一致，也可能不在同一范围内，因此在预测中使用这些极值将导致结果失真。

那么，我们能做些什么来克服这种情况呢？

简单来说，让我们考虑将所有的特征值转换到一个共同的尺度，这样它们就可以被算法有效地使用，以达到准确的预测。

下一个问题是，我们如何将这些单独的特征值转换成一个共同的尺度，我们是否有任何现有的方法或途径，以及我们是否有任何替代方案可供选择？

有多种方法可以实现这一点，我们可以选择其中的任何一种，只要单个特征的值与一个共同的尺度正确对齐。

*   解决这个问题的简单方法是用列表中的最大值除该特性的单个值。通过这种方式，我们可以始终使单个特性的值落在 0 和 1 之间，这样就可以很容易地将它们拉到一个通用的范围内，以便更好地建模。

例如，如果值(特征 1)在 1 到 9 的范围内，并且如果它们是 3、4、8、2、9

> 3 将被转换为 3/9= 1/3 = 0.33
> 
> 4 将被转换为 4/9=0.44
> 
> 8 将被转换成 8/9=0.88
> 
> 2 将被转换成 2/9=0.22
> 
> 9 将被转换成 9/9=1
> 
> 如果值(特性 2)在 1200 到 2000 范围内，并且如果它们是 1200、1350、1800、1650、2000
> 
> 1200 将被转换为 1200/2000 = 0.6
> 
> 1350 将被转换为 1350/2000 = 0.675
> 
> 1800 将被转换为 1800/2000 = 0.9
> 
> 1650 将被转换为 1650/2000 = 0.825
> 
> 2000 将被转换为 2000/2000 = 1

现在，这两个特性(特性 1 和特性 2)的所有值都转换到 0 到 1 的范围内，尽管它们各自的值有很大的不同。

*   另一种方法是用平均值来理解偏差，平均值将从数值中减去，所以公式可以是这样的

(X —平均值)/ R，其中 R 是可计算为(最大-最小)的范围。除了范围，我们还可以使用标准偏差，从统计学的角度来看，这样会好一点

让我们将这个概念应用于上面讨论的两个相同的特性

特征 1-3，4，8，2，9，平均值将是 5.2

> 3 被转换为(3–5.2)/7 =-2.2/7 =-0.314
> 
> 4 被转换为(4–5.2)/7 =-1.2/7 =-0.171
> 
> 8 被转换为(8–5.2)/7 = 2.8/7 = 0.4
> 
> 2 被转换为(2–5.2)/7 =-3.2/7 =-0.457
> 
> 9 被转换为(9–5.2)/7 = 3.8/7 = 0.542
> 
> 特征 2–1200，1350，1800，1650，2000，平均值为 1600
> 
> 1200 被转换为(1200–1600)/800 =-0.5
> 
> 1350 被转换为(1350–1600)/800 =-0.3125
> 
> 1800 被转换为(1800–1600)/800 = 0.25
> 
> 1650 被转换为(1650–1600)/800 = 0.0625
> 
> 2000 转换为(2000–1600)/800 = 0.5

因此，总体而言，这两个特性的值被转换到-0.5 到 0.5 的范围内，因此可以在相同的范围内工作。

特性可以无限扩展吗？

现在我们清楚了如何标准化不同比例的特征，这使得建模工作变得简单易行。然而，总会有下一个问题，如果特征的数量更多会发生什么，它将如何影响现有算法的性能？我们能试着让它变得简单和容易吗？

为了回答这些问题，让我们更仔细地看看建模。特征的数量必须是最佳的，以避免数据的欠表示(也称为欠拟合)或过表示(过拟合)。欠拟合可能不会导致准确的预测，而过拟合可能会使模型更特定于给定的数据集，并可能使其难以推广和拟合其他数据集，因此设置正确的特征计数是获得干净模型的关键，该模型可以合理地确保良好的预测。

**最终意见**

总的来说，任何模型的准确性取决于所涉及的特征的数量，我们拥有的特征越多，我们可以预测的就越准确，有时它可能会损害模型的一般性质。

真正的挑战出现了，如果我们需要缩短特征的数量，有像主成分分析(PCA)这样的技术可以帮助将给定的 n 个特征转换为减少的 k 个特征，使模型更通用而不是变得偏斜。另一种方法是尽量减少与特性相关的值，这样我们就不需要减少特性的数量。

是的，拥有更多的特征可以带来更好的预测，但我们需要小心确保我们不局限于一个特定的区域，而影响模型的更广泛/通用的使用。