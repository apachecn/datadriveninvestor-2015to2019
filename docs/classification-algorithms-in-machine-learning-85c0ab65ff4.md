# 机器学习中的分类算法…

> 原文：<https://medium.datadriveninvestor.com/classification-algorithms-in-machine-learning-85c0ab65ff4?source=collection_archive---------0----------------------->

[![](img/7d2857aa64736b060e22612756ac4fa0.png)](http://www.track.datadriveninvestor.com/1B9E)

# **什么是分类？**

分类是将我们的数据分类到所需的不同类别的技术，我们可以为每个类别分配标签。

***分类的应用有:*** 语音识别、手写识别、生物特征识别、文档分类等。

## 分类器可以是:

***二元分类器:*** 只有两个不同类别或两种可能结果的分类

例如:男性和女性

示例:垃圾邮件和非垃圾邮件的分类

例如:书籍作者的分类

例子:积极和消极的情绪

***多类分类器*** :具有两个以上不同类别的分类。

示例:土壤类型的分类

示例:作物类型的分类

示例:歌曲/音乐中情绪/情感的分类

# 1).朴素贝叶斯(分类器):

朴素贝叶斯是受贝叶斯定理启发的概率分类器。在一个简单的假设下，属性是条件独立的。

![](img/0e465e60cd9246e696061d40b45e9b18.png)

fig. Naive Bayes Theorm.

通过导出最大后验概率来进行分类，最大后验概率是最大 P(Ci| **X** ),上述假设适用于贝叶斯定理。这种假设通过只计算类别分布大大降低了计算成本。尽管这种假设在大多数情况下是不成立的，因为属性是相互依赖的，但令人惊讶的是朴素贝叶斯却表现得令人印象深刻。

朴素贝叶斯是一种实现起来非常简单的算法，并且在大多数情况下都获得了良好的结果。它可以很容易地扩展到更大的数据集，因为它需要线性时间，而不是像许多其他类型的分类器那样通过昂贵的迭代近似。

朴素贝叶斯会遇到一个叫做零概率问题的问题。当特定属性的条件概率为零时，它无法给出有效的预测。这需要使用拉普拉斯估计器来明确地解决。

***优点:*** 该算法需要少量的训练数据来估计必要的参数。与更复杂的方法相比，朴素贝叶斯分类器非常快。

***缺点:*** 众所周知，朴素贝叶斯是一个糟糕的估计量。

## **实施步骤:**

*   初始化要使用的分类器。
*   训练分类器:scikit-learn 中的所有分类器都使用 fit(X，y)方法来拟合给定训练数据 X 和训练标签 y 的模型(训练)
*   预测目标:给定一个非标签观察值 X，predict(X)返回预测的标签 y。
*   评估分类器模型

# 2).支持向量机:

***定义:*** 支持向量机(Support vector machine)是将训练数据表示为空间中的点，这些点被尽可能宽的清晰间隙分成类别。然后，新的例子被映射到相同的空间，并根据它们落在差距的哪一边来预测属于哪个类别。

***优点:*** 在高维空间有效，在决策函数中使用训练点子集，所以也是内存高效的。

***缺点:*** 算法不直接提供概率估计，这些都是使用昂贵的五重交叉验证计算出来的。

![](img/8624625a9992df044ce3119e989fb9f1.png)

fig. SVM

***图表描述:***

超平面的例子
H1 不是一个好的超平面，因为它没有把类分开
H2 分开了，但只有很小的间隔
H3 用最大的间隔(距离)把它们分开
([https://en.wikipedia.org/wiki/Support_vector_machine](https://en.wikipedia.org/wiki/Support_vector_machine)

## SVM 的参数

在构建 SVM 分类器时，我们可以使用三个主要参数:

*   内核类型
*   伽马值
*   c 值

# 3).k-最近邻(KNN):

在输入参数空间中，kNN 通过对象邻居的多数投票对对象进行分类。该对象被分配到其最近邻居中最常见的类别 **k(由人指定的整数)。**

它是一个**非参数的，懒惰的算法**。它是非参数化的，因为它没有对数据分布做任何假设(数据不必呈正态分布)。它是懒惰的，因为它不真正学习任何模型并对数据进行概括(它不训练输入 X 给出输出 y 的一些函数的一些参数)。

所以严格来说，这并不是真正意义上的学习算法。它只是根据**特征相似度**(特征=输入变量)对对象进行分类。

![](img/934efd78fdf5b5014850aa3f4c4a1a4e.png)

fig, classifying new example depending on Training instance distance

根据每个点的 k 个最近邻的简单多数投票来计算分类。

***优点:*** 该算法实现简单，对有噪训练数据具有鲁棒性，在训练数据较大的情况下有效。

***缺点:*** 需要确定 K 的值，计算成本高，因为需要计算每个实例到所有训练样本的距离。

# 4).决策图表

给定一个数据的属性及其类别，决策树产生一系列可用于对数据进行分类的规则。

***描述:*** 决策树，顾名思义，用树状模型进行决策。它根据输入变量中最重要的差异将样本分成两个或多个同类集合(叶)。为了选择微分器(预测器)，算法考虑所有特征并对它们进行二进制分割(对于分类数据，由 cat 分割；对于连续，选择一个截止阈值)。然后，它将选择具有最小成本(即最高精度)的一个，并递归地重复，直到它成功地将数据分割到所有叶子中(或达到最大深度)。

![](img/ee9cb6586d9cca36955632ec88ed9c0e.png)

fig. Tree like representation of datain Decision tree

***优点:*** 决策树易于理解和可视化，需要很少的数据准备，可以处理数值和分类数据。

***缺点:*** 决策树可以创建复杂的树，但不能很好地概括，并且决策树可能不稳定，因为数据的微小变化可能导致生成完全不同的树。

# 5).随机森林

随机森林是一个集合模型，它生长多棵树，并基于所有树的“投票”对对象进行分类。即一个对象被分配给在所有树中拥有最多投票的类别。通过这样做，可以缓解高偏差(过拟合)的问题。(—来自 Kaggle)。

![](img/d5ef9c4239e4b15ef31556dde5a24241.png)

fig. Random forest

随机森林分类器是一种元估计器，它在数据集的各种子样本上拟合多个决策树，并使用平均值来提高模型的预测准确性并控制过度拟合。子样本大小始终与原始输入样本大小相同，但样本是替换绘制的。

## 射频的优点:

*   它可以处理高维的大数据集，输出变量的**重要性，有助于数据的挖掘**
*   能够在保持准确性的同时处理缺失数据

## 射频的缺点:

*   可能是一个黑盒，用户对模型的功能几乎没有控制

***优点:*** 减少过拟合，随机森林分类器在大多数情况下比决策树更准确。

***缺点:*** 实时预测慢，难以实现，算法复杂。

# 来自 DDI 的相关帖子:

[](https://www.datadriveninvestor.com/2019/01/23/deep-learning-explained-in-7-steps/) [## 用 7 个步骤解释深度学习——数据驱动投资者

### 在深度学习的帮助下，自动驾驶汽车、Alexa、医学成像-小工具正在我们周围变得超级智能…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2019/01/23/deep-learning-explained-in-7-steps/) [](https://www.datadriveninvestor.com/2019/01/23/which-is-more-promising-data-science-or-software-engineering/) [## 数据科学和软件工程哪个更有前途？-数据驱动型投资者

### 大约一个月前，当我坐在咖啡馆里为一个客户开发网站时，我发现了这个女人…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2019/01/23/which-is-more-promising-data-science-or-software-engineering/)