# 强化学习教会了我关于生活的 7 件事

> 原文：<https://medium.datadriveninvestor.com/10-things-reinforcement-learning-has-taught-me-about-life-bea07bf99b35?source=collection_archive---------8----------------------->

![](img/bf839cb09c649d68327174ac7e1b0194.png)

Photo by [Luca Bravo](https://unsplash.com/@lucabravo?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

强化学习是机器学习的一个领域，本质上是与一个代理一起工作，在给定的情况/环境下找到达到某个目标的最佳路径。该代理通过采取使奖励函数最大化的行动来实现最佳策略或路径。如果你在机器学习方面没有任何经验，我更倾向于认为你觉得这个定义非常直观。当我在过去的一周里学习它的时候，我也有同样的感觉。强化学习听起来基本上和我的生活一样。我是一个暴露在这个叫做地球的环境中的代理人，对于我所处的每一个状态或时刻，我都可以采取某些行动来更接近我为自己设定的目标。哇哈哈。实现。无论如何，这里有几个强化学习的概念，我能够思考并用来强化我的类比(尽管看单词 play)。

# 课程

## 没有转移概率和报酬分布

在机器学习的背景下，马尔可夫决策过程由转移概率和报酬分布组成。更简单地说，这意味着对于你作为代理人的所有行为和状态，你已经知道你将为某些步骤获得多少奖励，并且你已经知道下一步该做什么。本质上，强化学习的目的是在你不知道 MDP 的情况下解决马尔可夫决策过程(MDP)。

[](https://www.datadriveninvestor.com/2019/03/03/editors-pick-5-machine-learning-books/) [## DDI 编辑推荐:5 本让你从新手变成专家的机器学习书籍|数据驱动…

### 机器学习行业的蓬勃发展重新引起了人们对人工智能的兴趣

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2019/03/03/editors-pick-5-machine-learning-books/) 

如果你看看你的生活是如何运作的，你只能努力成为一个神。我的意思是，你永远不会完全知道人生的转移概率和回报分布。简而言之，你看不到未来。但是，这并不意味着你应该停止想未来。这里的目标是扩大你的控制范围，了解你目前所处的位置可以做什么。努力解决 MDP！

## 州

在强化学习中，我们有一个初始状态，在我们采取的每个行动之后，我们被置于另一个状态。对于我们所做的每一个决定，无论是好是坏，我们基本上都朝着某个目标前进了一步。这听起来有点令人生畏，但是请记住，我们每天要做大约 10，000 个决定。我可以自信地说，我们在潜意识中做出的好决定比坏决定多。因此，一个不好的行为不会毁了你每天的算法。

## 探索与开发

在 RL 中，有探索的概念，也有剥削的概念。在 RL 的上下文中，探索意味着尝试随机的事物。这就像一个青春期前的孩子寻找爱好。另一方面，剥削是利用人们已经知道的东西。如果你现在已经有了一个好方法，那就去做吧。

对于具有更大复杂性(就像生活一样)的 RL 问题，通常理想的解决方案实际上是通过使用 Q-Learning 方法来最佳地平衡探索和利用之间的权衡。我想这也是生活的一部分。你必须在以冒险的方式探索和理解我们可以利用的以前的解决方案之间取得良好的平衡(不要重新发明轮子)。

## 有目标

我们到底想用 RL 解决什么问题？如果头脑中没有设定目标，我们采取的行动可能会变得毫无意义或效率低下。如果代理甚至不知道去哪里，它将无法解决最佳路径和奖励分配。即使这还不是你的主要生活目标，也要朝着你设定的目标去做。这样，你至少会找到一个问题的答案，也许不是你试图解决的问题，以及一个伴随的反馈循环来帮助你继续前进。(我猜这是无监督学习部分哈哈)

## 动态规划

RL 中实际使用的迭代算法，主要是值迭代和策略迭代，源于理查德·贝尔曼引入的动态规划(DP)概念(贝尔曼方程)。因此，RL 是用 DP 的思想构建的。动态编程实际上是通过递归地解决更大问题的子问题来工作的。“贝尔曼优化原则”本质上是说，一个问题的子结构必须首先得到最优解决，它也将转化为更大问题的最优解决。

在终身学习中，有几个阶段。对于大多数高级的东西，在能够完全掌握难的东西之前，有一些先决知识你必须完全理解。确保你先把你的基础打好！

## 最优策略

老实说，框架非常重要。

## 贴现因素

在不必深究数学公式的情况下，我们试着简单解释一下价值迭代算法(贝尔曼的宝贝)中关于价值函数的独特之处。关于这个价值函数，你需要知道的最重要的事情是，它不只是对整个行动空间的回报分布求和。它还考虑了未来的步骤和折扣因子(γ)。之所以称之为贴现，是因为当前步骤(有报酬分布的步骤)比贴现的未来步骤被赋予了更大的权重。

从中我们可以得到的是，当下还是最重要的。过去的已经过去了，未来不会完全在我们的掌控之中。我们此刻做出的每个决定都是我们力所能及的。因此，这里的关键概念是，我们应该现在就做出决定，这很可能会增加我们在下一个状态/时刻采取更好行动的可能性。专注于现在！