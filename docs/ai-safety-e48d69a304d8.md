# 人工智能安全:重要性和可能的解决方案

> 原文：<https://medium.datadriveninvestor.com/ai-safety-e48d69a304d8?source=collection_archive---------35----------------------->

[![](img/e65db49f3fad9ff3881ec3863861cd37.png)](http://www.track.datadriveninvestor.com/1B9E)

如果 AI 的问题是解决世界饥饿，你认为它会提出什么解决方案？分配财富？改善第三世界国家的教育？

也许吧。

确保没有人挨饿的最快方法是让任何人都不存在。如果唯一的目标是消除饥饿，那是最好的解决方案。如果这个问题被交给一个能够执行解决方案的程序，我们将会面临一个大问题。太吓人了。

安全可以定义为“远离不可容忍的风险”。 AI 安全是防止**技术奇点**的思想。奇点是一种假设，即当人工智能变得比人类更聪明时，它会以超出我们理解的速度自学，改变我们所知的文明和世界。最终，我们将有能力创造出比我们更快更聪明的人工智能。由于我们很可能不想阻止它或无法阻止它，我们必须确保我们作为一个物种，减轻风险。

但首先，让我们后退一步。

## 人工智能的类型

1.  **窄/弱 AI (ANI)** 描述了被设计来处理一个特定任务的 AI，例如将肿瘤分类为恶性或良性。甚至 Siri 也是一个弱人工智能，因为它在有限的预定义功能范围内运行。Siri 不是真正的智能，它没有自我意识。这就是我们今天的处境。
2.  **通用/强 AI (AGI)** 指能够像人类一样解决问题、做决策的系统。比如你在电影和电视剧中看到的人工智能。
3.  **超智能 AI (ASI)** 本质上和一般 AI 一样，比最有天赋的人类还要聪明。一旦我们实现了人工智能，这就不远了。

我认为没有足够多的人了解计算机实际上有多强大。

我们的生物神经元的工作频率约为 200 赫兹，但现代晶体管的工作频率为 2 千兆赫兹。神经元以大约 100 米/秒的速度在轴突(大脑中的神经末梢)中传播，大约是音速的三分之一。相比之下，计算机以光速传输信息。所有这些加起来，一台计算机一周能做 20000 年的人类工作。

谷歌的 DeepMind 创建了一个名为 **AlphaGo** 的计算机程序，它玩棋盘游戏围棋。AlphaGo 是用强大的业余打法训练出来的。然后，它与自己作对，每次都从过去的错误中吸取教训。AlphaGo 的第一场正式比赛是对阵 3 次欧洲冠军，它以 5 比 0 获胜。接下来，它与世界冠军比赛，并以 4-1 获胜。另一个程序被创造出来，叫做 AlphaGo Zero。与 AlphaGo 及其变种不同，AlphaGo Zero 只通过与自己对弈来训练。它只被赋予了规则，没有游戏性或人类互动。 **AlphaGo 零胜 AlphaGo 100–0，只需要 3 天的训练就能击败 alpha go。**

显而易见，但却被极大忽视的一点是**比*拥有更多非人类的知识。如果技术奇点出现，用不了多久我们就会代表少数智能。当这种情况发生时，我们的价值是什么？许多人认为人工智能没有理由伤害我们，但也没有理由让我们留在身边。这就像人类碾碎一只蚂蚁。***

当人们想到管理机器人时，他们通常会想到阿西莫夫的机器人定律。

1.  机器人不得伤害人类，也不得坐视人类受到伤害。
2.  机器人必须服从人类给它的命令，除非这些命令与第一定律相冲突。
3.  机器人必须保护自己的存在，只要这种保护不违反第一或第二定律
4.  一个机器人不能伤害人类，或者，通过不作为，允许人类受到伤害(第零定律-随后添加)

有趣的是，阿西莫夫创造了许多关于这些法律漏洞的故事，以及为什么它们不起作用。

该领域的一些人提出了授权(授权机器人做出自己的决定)的想法，而不是无助(制定硬性规定来限制机器人的权力)。这是有意义的，因为如果我们的智力对 ASI 来说是贫乏的，那么为远比你优越的东西创造规则将是低效的。AlphaGo 的一些棋步完全违背了我们所知道的游戏规则。我们应该更加关注人工智能如何做出决定，而不是它做什么。

## 授权

授权可以被描述为*“一个信息理论的量，它捕获了一个代理在多大程度上控制了它所能感知的世界”*。赫特福德郡大学的研究人员亚历山大·s·克柳宾提出了授权的概念，目的是提供动机，作为迈向更复杂行为的垫脚石。总而言之，人工智能找到人类的价值，并找到最大化它们的最佳方式，比人类限制程序的行动更安全。

著名的计算机科学家 Stuart Russell 教授因他的人工智能教科书而闻名，他提出了创造更安全的人工智能的 3 个原则

1.  机器人的唯一目标是最大限度地实现人类价值。
2.  机器人最初不确定这些值是什么。
3.  人类行为提供了关于人类价值的信息。

这些原则允许人工智能根据我们自己的人类价值观创造自己的规则。

但也许没那么简单。

其他研究人员说，如果不太可能，人工智能也有可能在我们给他们的任何法律中找到漏洞。如果这不是真的，最终没有什么可以阻止超级智能人工智能违反我们赋予它的任何法律。

## 更多资源:

[关于授权的详细文章](https://www.frontiersin.org/articles/10.3389/frobt.2017.00025/full)

[Stuart Russell 的 TED 演讲:创造更安全人工智能的 3 个原则](https://www.ted.com/talks/stuart_russell_3_principles_for_creating_safer_ai)