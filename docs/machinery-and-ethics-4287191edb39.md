# 机械与伦理

> 原文：<https://medium.datadriveninvestor.com/machinery-and-ethics-4287191edb39?source=collection_archive---------4----------------------->

[![](img/6e30e6c6d767aaa94b2bce97c57b7b87.png)](http://www.track.datadriveninvestor.com/1B9E)![](img/e471e095038a39613b8df30dcf76e18d.png)

没有人能逃避这样一个事实，即试图对人和机器之间的现有联系进行立法将会导致一个严重的问题，即把我们的习俗和道德转移到 T2 伦理领域。事实上，认为人类和类人机器人之间的任何社会契约不会通过**人权**得到反映是愚蠢的，尤其是当类人机器人本身完全是人造的时候。

出发点是分析哪些是机器获得的**固有人权**，以便能够考虑哪一方面与**道德**有关。因此，简化这篇文章的一个方法是假设 1948 年的人权宪章可以构成伦理原则的一部分，它不仅定义了人类，还定义了人性的概念。

[](https://www.datadriveninvestor.com/2018/06/28/protectionism-politics-economic-turmoil/) [## 保护主义、政治和经济动荡——数据驱动的投资者

### 美国股市昨日出现 400 多点的大幅反转，为未来的事情发出了警告信号。市场…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2018/06/28/protectionism-politics-economic-turmoil/) 

> 1948 年的《人权宪章》可以成为界定人类概念的道德原则的一部分。

因此，如果意图是考虑机器获得权利，则必须首先考虑哪些权利根据先天能力属于它们。这是因为它们本身是在人类社会中出现的。另一方面，由于**道德戒律**而授予新的权利可能会与**最基本权利**的**伦理**相矛盾。

在此之前，为了我们的分析，我们研究了世界人权宪章中信息系统的各种幻影。当我们看到，首先，它们没有得到明确的反映，我们必须促进对
权利本身的重新定义，以更新它们，适应我们 2019 年生活的时代。

# 第 1 条引起的问题

第一条。人人生而自由，在尊严和权利上一律平等。他们被赋予理性和良知，并应以兄弟关系的精神相对待。

似乎人权是为那些被赋予了强迫友爱行为作为回报的自由的人所认可的。这一点至关重要，因为机器是从惯性中诞生的，所以它不能被认为是自由的。那些没有*自由*的人没有*责任*，因此不能选择表现得友好。

工程师解决这个问题的方法很简单:人类自己也可能不自由，而且就科学而言，不可能确定他的惯性公式。然后，正如机器会发生的那样，它会有足够复杂的行为被认为是*随机的*。

然而，在机器被认为与我们平等之前，这一理论必须被视为第一个最基本的约束:它不仅必须获得复杂的随机能力(在(图灵 1996 年)中要求)，而且还必须能够确保它将倾向于或**接受我们的道德戒律**。它必须和其他人类友好相处。事实上，这台机器必须克服**的举证责任**，不仅要通过**图灵测试**，还要通过*重新融入和适应能力测试*以应对它失败的时候(“宽容和耐心比仅仅漠不关心要深刻和有效得多”，达赖喇嘛)。

# 第 2 条引起的问题

**第二条。** *人人有资格享受本宣言所载的一切权利和自由，不分种族、肤色、性别、语言、宗教、政治或其他见解、国籍或社会出身、财产、出生或其他身份等任何区别。此外，不得基于一个人所属国家或领土的政治、司法或国际地位而有所区别，无论该国或领土是独立的、托管的、非自治的还是受到任何其他主权限制的。*

这篇文章反映了一种巨大的恐惧，当我们有许多专家，而他们每个人都有不同的道德准则，这可能会影响整体的共存。这是一个国家希望承认它正在克服违反第 1 条的情况，而不是另一个标准更严格的国家。在这种情况下，我们将有一个违反第二条。

> 在一个决策过程中间，一眼看去，根本分不清惯性和活人。

一件作品的生产能力及其在测试中的道德克服别无选择，只能符合国际公认的标准框架。在这个框架内，也有必要研究这样一种可能性，即一个国家可能建立一个有智慧但不道德的制度(它不考虑 T21 国际友爱)，一个反社会的制度(它可能成为反对友爱的武器)，或者一个直接不道德的制度(它利用它的智慧毒害权利)。

标准必须具备这三种可能的结果，才能确定各国应该施加何种政治压力来规范或解决这种冲突。

# 3、12 和 27 中的安全和隐私问题

**第三条。** *每个人都有生命、自由和人身安全的权利。*

承认机器是可以共存的实体会带来巨大的风险，那就是著名的阿西莫夫**(*规避*，1942)规则可能会被违反。主要是因为它甚至不可能强迫一台机器识别什么行为与人的安全有关，或者以任何其他惯性方式。别的不说，因为在一个决策过程中间，一眼看去，是无法区分*惯性*和*活体*的。
这就是为什么虚拟实体的形象永远不能做出可能影响**第三方**生活的决定。换句话说，正是在这一点上，我们必须开始考虑谁是第三方。**

**在软件生命周期中，系统中涉及到四个法律实体。他们是**用户**、**管理员**、**开发者**和**所有者**。*判例法*和 *IT 标准*帮助我们清楚地定义这四个角色:**

****用户**是使用该产品的个人。利用资源的特权制度可能会落到它头上。后者不拥有产品的知识产权，但拥有上传到产品上的**内容**。**

****管理员**是请求执行产品**创建**的个人。他拥有使其有效的资源。它把**用户**作为它的*客户*，它与他们有一种与所提供的服务相关的*客户-服务器关系*。是**的知识分子*所有者*用不同的*外观和感觉*与**接口，因为它是设计其开发需求的人。**

**信息系统的**所有者**实际上是其*产业所有者*。这是负责**支持**产品创建的个人，其*客户*是**管理员**。**管理者**和**所有者**之间的关系仅仅是商业关系，基于预算的履行。**

****开发人员**是开发信息系统的个人。它的*客户*就是**老板**，他们的关系通常是劳动或商业关系。它是信息系统的**结构**以及内容和常规接口的*固有所有者*，或者不是由
管理员**或**用户**定义的。****

**根据**第 27 条**的规定，我们认识到这些法律实体各自的权利，在以下部分中，我们观察到有关数据保护的冲突。**

# **关于荣誉的法律**

****第十二条。** *任何人的私生活、家庭、住宅或通信不得加以任意干涉，他的荣誉和名誉也不得加以攻击。人人有权享受法律的保护，以免受这种干涉或攻击。***

**与本文档相关的荣誉或声誉是指我们可能使用的信息系统。假设系统能够按照友爱的规则行动，所使用的语义网络可以被利用，这样系统的所有者就可以干涉用户的私人生活
。**

**这就是为什么任何包含个人信息痕迹的信息系统都必须包含一种机制，以防止软件生命周期中涉及的不同个人访问这些信息。**

> **一个由不认可用户荣誉的过程创建的信息系统注定会被认为是一个不道德的系统。**

**也就是说，构造一台可以访问敏感信息，同时又有足够的智能来造成伤害的机器，必须包括受控的许可标准，这样每个角色只能访问与其对应的精确信息，并且永远不会不相称。**

> **无罪推定是人权中唯一一个承认超越惯性的条款。**

**否则，一个从不承认用户荣誉的过程中创建的信息系统注定最多被认为是一个不道德的系统。或许在研究了**的所有者**、**的开发者**或**的管理者**在承认自己清白的同时，是否能发现任何可能的利益( *QUID PRODES* )后，可以认为该系统**反社会**。显然，简而言之，如果在创造的过程中有**被证明的不正当的意图**，这个系统必须被直接认为**不道德**。**

# **假定无罪和其他保证的可能冲突**

****第十一条。*****【1】****任何被控犯有刑事罪的人，在他已获得辩护所需的一切保证的公开审判中，在依法被证明有罪之前，有权被视为无罪。* ***(2)****任何人的任何行为或不行为，如果在犯罪发生时根据国内法或国际法不构成刑事犯罪，则不得被判犯有刑事罪，也不得处以比实施刑事犯罪时适用的刑罚更重的刑罚
。***

**一旦我们为过于智能的机器可能造成的损害奠定了基础，无论是由于其个体行为、国际冲突，还是参与其开发的集体的私人使用，我们现在都必须更新一项基本权利的最正确措辞，然后才能表明它在任何寻求将机器人纳入其社会的文明社会中是如何受到影响的。**

**源自罗马法的一项法律公认的基本原则是承认无罪推定。在我看来，这是《人权》中唯一一篇承认超越惯性的文章。**

> ****用户**的义务是向**第三方保证**损害保险**。****

**一个人想要承认一个人的清白，这意味着他的清白得到了承认:这个人有能力以违背社会利益的方式行事，即使这违背了他的职责。社区也有权以落实个人权利的方式解释这些事实。**

**第三，社会将承担举证责任，也就是说，不遵守人类义务是基于正式的证明而不是回忆，所以语言必须用来表明什么是合法的，什么是不合法的。换句话说，**必然有**通过语言表达的**道德**的认同。集体的义务应当是明确和证明被告没有履行这一义务。**

**打破这个想法，我们看到，因此在个体中没有能力在集体面前展现他的所有历史(因为违反了先前的戒律)，另一方面，他自己对他的利益是否是*友爱的*(违反了第一部分)。**

**因此，系统内成员的**警惕义务**(也在**第 28 条**中得到含蓄承认，该条承认有必要行使权利)也得到承认，使他们对权利的实现负责。换句话说，我们对一些法律实体承认**无罪推定**，对另一些法律实体承认**警惕义务**，这与**合理怀疑**相矛盾。因此，这就是**合理怀疑**对个人的作用:信息系统如果真的要被卷入系统内而不影响不同的人的冲突，就必须被承认为系统内的一个独立的法律实体。**

**这意味着**用户**有义务向**第三方**提供**损害保险**，以防信息系统影响与其生命周期不直接相关的个人。本保险应在其适用范围内，采用**警惕义务**通过与受影响者的适当调解，避免在法庭上发生冲突。**

# **衡量系统信息不道德的程度**

***给定一个道德准则，在一个社会选择情境中，个人提交自己的偏好排序，结果是一个集体偏好排序，如何衡量集体排序对道德准则的偏离？
如何衡量个人对集体道德准则的偏离？*(罗西 2016)**

**在这一点上，有必要捍卫某种论点，假设一种能够确定何时必须干预信息系统的规则。让自然语言适应自动机的尝试是众所周知的(Tomita 1984)。它的局限性是基于这样一个事实，即字母表必须总是有限的，而要做到这一点，没有一种通用而有效的原始语言能够通过图灵测试(Saygin，Cicekli 和 V. 2000)。**

**这就是为什么应该努力猜测哪些语言技能是通过实用主义、通过交流的本能获得的，哪些是文化的产物。**道德**不难想象，它应该是本能(用语言(Koerner 1998))获得的技能的一部分。然而，在考虑道德偏差之前，有必要了解软件在开发能力过程时所经历的生命周期。
在这篇文章中，我们已经讨论了一个系统是如何被道德限制的，因此出现了非道德的、反社会的和不道德的词语。现在让我们来详细说明当一个主体必须接受一个打破其原有模式的新概念时会发生什么。为此，我们将基于
非常著名的俱吠罗-罗斯模型，也就是说，在接受之前有四个阶段，我们必须接受/采用我们模型中的新概念。为了适应接受集体语言的想法，我称之为 **AGSF** 系统。**

**一个 **采集**。在第一阶段，服务器从客户机接收大量信息，如 Hockett 的设计特征所表达的那样(Hockett 1960)。有这么多信息，其中大部分必须被否认，被鄙视。然而，新的道德准则也可能被忽视，因此必须考虑拒绝否认。**

**G通用化。在模型中输入新的中断数据的下一个必要步骤是转换规则中的一些数据。内在的和适当的模式的破裂意味着在面对一个持久的概念时克服一种兴奋，这个概念就是愤怒。当整个系统正在煽动打破模式，通过这一步创造了一个新的更好的规则。**

**SS**规格**。在认识到模型之后，有必要经历一个合并特定案例的阶段。这就像当您必须区分单词和大小写时，例如区分复数，一般规则可能是在末尾包含一个字母 s，但随后指定新的规则来缩放模型。这时，不同的模型通过它们的不变量显示它们的契约，并继续协商。这种操作就像一台梯度推进机，即(Friedman 1999)。**

**F反馈在重新思考一个新概念的整个过程的最后，一个人必须假设新的现实，玩弄它，并思考它，就好像它是代理人自己一样。这一过程对于在验收前完成整个过程至关重要。透明度的主要敌人是对信息系统中每一个行为的抑制，也就是有某种形式的伪装。**

**事实上，经验告诉我们，一个班级中的每个学生(首先是学习一种文化非常不同的新语言，就像一个欧洲人学习汉语一样)在学习一种心态和吸收新语言之前都必须经历这些阶段。**

**我们可以假设:**

*   **如果一个学生停留在第一阶段，我们会说他有一个边缘问题，因为他不能以正确的顺序组织他的想法。**
*   **如果他停留在第二阶段，我们会说他有*困惑型*问题，因为他无法保留想法，也不欣赏细微差别。**
*   **如果他停留在第三阶段，我们会说他有*躁郁症*的问题，因为当你向他解释某事时会太慢，似乎他得到的是不相关的解释。但如果加速解释，他认为概念太多，无法浓缩
    它们。**
*   **如果一个学生停留在第四阶段，我们会说他有*多动症*的问题，因为他感觉不到教学本身的动力。**

**同样地，必须明白，在某种程度上，老师可以对学生陷入某个阶段的能力负责。不做好编程，不知道怎么教，不知道怎么解释，不知道怎么激励，可能是老师的错。**

**然而，由于机器是如何被引入道德自我评估的，我们必须假设暴力的作用将与意识相关联——如果它是有限的，它可以让我们更好地控制结果。**

**正是在这个精确的时刻，一种测量道德缺失的能力已经可以被引入。必须考虑到**道德**的每一项衡量都受制于机器是如何制造的:而**举证责任**则落在*设计者* ( **管理者**、**所有者**和**开发者**)身上，以证明机器不是**不道德的**、**反社会的**或**不道德的**。**

**但是还有一个小代数来简化我们的测量:**

*   **如果**用户**显示他的**道德机器**扰乱了输入的大量信息，那么该机器将被视为**道德机器**。**
*   **如果**用户**证明他的**非道德机器**不能从特殊情况中归纳，那么该机器将被认为**反社会**。**
*   **如果**用户**证明他的**反社会机器**不能用其模型提升坡度，那么这台机器将被认为**不道德**。**
*   **如果**用户**展示他的**不道德机器**不透明，那么该机器将被认为**危险**。**

**因此，通过一些研究机器相关性的工具，比如将在最初的[文章](https://archive.org/details/MachineryAndEthics)中披露的道德测试，当机器比设计者告诉的更差时，**用户**将会演示。显然，如果**危险**意味着**退款**，设计者会认为建造道德机器更有趣。**

# **感谢**

**作者必须感谢著名的英语教师 Nicolas Montalban，他建议改进解释人权适用的英语。**

# **原始文章的参考文献:**

*   **弗里德曼，J. 1999。贪婪函数逼近:一个梯度
    助推机。即时消息。**
*   **霍克特，约 1960 年。言语的起源。科学美国人
    203:89–90。**
*   **科尔纳，1998 年。走向从洛克到露西的“萨皮尔霍夫假说”的“完整谱系”。赞美 455。**
*   **明斯基，硕士，1991 年。逻辑对类比或符号
    对联结主义或整洁对邋遢。艾杂志
    12:34–51。问题 2。**
*   **庞加莱，H. 1908。科学与方法。多佛图书公司。**
*   **罗西，F. 2016。道德偏好。IJCAI。
    赛金，a；奇切克利岛；和 v . a . 2000。图灵测试:50 年后
    。思想与机器 10:463–518。**
*   **富田，M. 1984。一种高效的自然语言全路径解析算法
    。国防高级研究计划局。**
*   **图灵，A. 1996。智能机械，一种异端学说。数学哲学 4:256–260。**
*   **翁，J. 2015。作为涌现有限自动机的大脑:一个理论和三个定理。国际智力杂志
    科学 5:112–131。**