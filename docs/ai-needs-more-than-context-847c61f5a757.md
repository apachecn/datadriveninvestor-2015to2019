# 人工智能需要的不仅仅是背景

> 原文：<https://medium.datadriveninvestor.com/ai-needs-more-than-context-847c61f5a757?source=collection_archive---------3----------------------->

![](img/cd3d120efaa7a835b47e4a40470eebc6.png)

18 个月前，我写了一篇名为“ [AI，context](https://chatbotslife.com/ai-a-roadmap-for-context-65ef01fe4567) 的路线图”的文章，因为当时人们(例如谷歌)说“他们做了 Context”，而实际上他们只是做了 Context 的一小部分。这篇文章变得非常受欢迎，现在人们不再说他们做上下文(因为他们还没有)。

现在我意识到“背景”是一个突现的属性——“突现”是一个物理学家用来描述从基本定律衍生出来的东西的术语——重要的是基本定律。

背景实际上是我们对我们的感知输入数据运行模型(模拟)以理解和预测未来的结果。重要的是模型，背景是我们解释模型的结果。让我们想象一下，我们与某人进行了一次对话，并开始创建一个正在说什么的模型——然后在对话进行到 5 分钟时，这个人说了一些不符合该模型的话——事实上，我们意识到我们误解了用户正在谈论的内容——我们必须迅速创建另一个不同的模型，并且实际上使用现有的上下文数据和新数据来创建它。结果将是一个新的模型和一个改变的上下文数据集。

因此，我们不断地运行这个模型，以便在新数据到来时检查它是否仍然有效，并且预测接下来会说什么。当我们听到下一个短语时，我们将它与我们预测的进行比较，并对模型和上下文数据进行适当的调整。

因此，我们作为人类观察到的不是现实，而是我们对感知输入数据的建模，我们将这些数据感知为现实。人工助手也应该有这些模型生成(模拟)，因为这创造了他们自己对现实的感知。如果下一个输入与预期不符，他们需要提出一个澄清问题，然后重建模型。这些模型还提供了另一个非常重要的组成部分——它为我们得出的结论提供了理由。传统神经网络的一个问题是，它们从数据中学习的方式使得特定答案的原因不可用——通常一些其他软件必须单独提供给定答案的原因。建立模型(可以从神经网络建立)可能是为答案提供推理的更结构化的方式。

我仍然相信语境的路线图，我认为我以前文章中的结论仍然有效——然而，我确实认为理解的模型更重要。

[![](img/4270b3e4285d19c2c93be3eb63673c5e.png)](http://eepurl.com/dw5NFP)